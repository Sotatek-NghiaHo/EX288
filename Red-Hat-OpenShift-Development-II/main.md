# Chapter 1.  Red Hat OpenShift Container Platform for Developers


![alt text](pic/1.png)
Control Plane

Kubernetes services
- etcd: the distributed key-value store, which Kubernetes uses to store configuration and state information about the containers and other resources inside the cluster.
- kube-apiserver: validates and configures cluster objects and provides the access point to the shared state of the cluster.
- kube-controller-manager: monitors etcd for changes and uses the Kubernetes API to apply changes to the cluster.
- kube-scheduler: selects the nodes where a workload must run.

## Red Hat OpenShift Concepts and Terminology

### Kubernetes Concepts

**Pods**  
A Pod is a collection of containers that share the same storage and network. Pods share the context by using Linux namespaces, cgroups, and other isolation technologies.

Each container in a pod usually contains applications that are more or less logically coupled.

**ReplicaSet**  
The ReplicaSet object indicates the number of pods that are available to attend a request. This object also ties all the pods replicas together so you can operate on them at the same time.

**Deployments**  
A Deployment contains the desired state of an application's pods and uses a ReplicaSet to achieve this desired state. Some changes in the application's state can be: creating pods, declaring a new state of pods, changing the number of pods, or rolling back to a previous Deployment revision.

**Service**  
A Kubernetes Service exposes a set of pods over a network. This abstraction allows internal or external clients of the application running on said pods to connect to them regardless of the actual state of the replicas or varying network IPs.

**Ingress**  
An Ingress exposes services inside the cluster to outside clients by using HTTP or HTTPS. A service ingress can also provide external URLs, load balancing, name-based virtual hosting, or SSL/TLS termination.

**Namespace**  
A Namespace can enable you to isolate resources, encapsulate objects under a unique name, and provide resource quotas.

**Custom Resource**  
Custom Resource (CR) allows extending the Kubernetes API. Custom resources represent entities other than the default ones in Kubernetes. Additionally, Custom resources interact with other cluster objects, regardless of whether those other objects are default or custom.

**Operator**  
An Operator is a custom Kubernetes controller that uses custom resources to deploy and manage applications. It takes high-level user configuration and acts to make the cluster match the desired state.

**Service Account**    
A Service Account is a special kind of account that does not correspond to an actual user, but it is used internally by cluster tools. It is useful for pods to connect to objects in the cluster, such as CI/CD pipelines, secrets, or external resources (outside of the namespace or the cluster).

**Storage Class**   
A Storage Class is a name that identifies a particular kind of storage defined by the cluster administrator. A storage class also defines its characteristics, such as backup policies, service level quality, or any other specification the administrator might choose.

**Persistent Volume**  
A Persistent Volume (PV), is a persistence storage unit offered by the cluster, independent of cluster nodes. This object holds information regarding the size, type, or ability to share storage.

**Persistent Volume Claim**  
Users claim the storage that a PV offers by using a Persistent Volume Claim (PVC). A PVC is a request to access a specific kind of storage of the required size. After acquiring the PVC, the storage is attached to the pods claiming it.

---
# Chapter 2.  Deploying Simple Applications

![alt text](pic/2.png)

![alt text](pic/3.png)

Deploying Applications by Using the Red Hat OpenShift Web Console
- Deploy an application by using a Git repository.
- Deploy an application by using an image from a container registry.

![alt text](pic/5.png)

![alt text](pic/6.png)


## Deploying Applications by Using the oc and odo CLIs



**Deploying Applications with the OpenShift CLI**   
For complex applications, you can use the oc apply command with the -f option. You provide a manifest file that defines the Kubernetes resources for your application.
```
[user@host ~]$ oc apply -f my-manifest.yaml
```
Deploying Applications with oc new-app

![alt text](pic/4.png)

To get a complete list of options and to see a list of examples, run the `oc new-app -h` command.

Example

![alt text](pic/8.png)

![alt text](pic/9.png)

You can delete resources that the oc new-app command creates by using a single oc delete command with the --selector option and the app label. The following command deletes the resources created by the previous oc new-app command:
```
[user@host ~]$ oc delete all --selector app=hello
```

Use the `-o` option to inspect resource definitions without creating resources.
```
[user@host ~]$ oc new-app \
-o yaml registry.example.com/mycontainerimage
apiVersion: v1
items:
- apiVersion: image.openshift.io/v1
  kind: ImageStream
  ...output omitted...
- apiVersion: build.openshift.io/v1
  kind: BuildConfig
  ...output omitted...
- apiVersion: apps/v1
  kind: Deployment
  ...output omitted...
- apiVersion: v1
  kind: Service
  ...output omitted...
kind: List
metadata: {}
```
**Exposing Applications Outside the Cluster.**  
Service resources are only accessible from within the cluster. To provide external access to your application, you can use the oc expose command.

The `oc expose` command creates a Route resource, a type of OpenShift-specific resource. This resource defines the port and protocol for access outside the cluster.

Syntax
```
oc expose [RESOURCE/NAME] [options]
```
Trong ƒë√≥:

- RESOURCE/NAME: lo·∫°i resource v√† t√™n (th∆∞·ªùng l√† service/<service-name> ho·∫∑c deployment/<deploy-name>).
- [options]: c√°c t√πy ch·ªçn ƒë·ªÉ ƒë·ªãnh nghƒ©a route ho·∫∑c service.


M·ªôt s·ªë option quan tr·ªçng

- `--name=<string>`
‚Üí ƒê·∫∑t t√™n cho resource m·ªõi ƒë∆∞·ª£c t·∫°o (v√≠ d·ª•: route).

- `--port=<port-name|number>`
‚Üí Ch·ªâ ƒë·ªãnh c·ªïng t·ª´ Service m√† Route s·∫Ω expose (n·∫øu Service c√≥ nhi·ªÅu port).

- `--target-port=<number>`
‚Üí Ch·ªâ ƒë·ªãnh ch√≠nh x√°c c·ªïng ƒë√≠ch tr√™n Pod container.

- `--hostname=<string>`
‚Üí ƒê·∫∑t hostname (FQDN) cho Route. N·∫øu kh√¥ng c√≥, OpenShift s·∫Ω generate.

- `--path=<string>`
‚Üí G·∫Øn path cho Route (v√≠ d·ª• /api).

- `--type=<string>`
‚Üí Lo·∫°i service mu·ªën expose (ClusterIP, NodePort, LoadBalancer).

- `--generator=<string>`
‚Üí Ki·ªÉu resource generator. M·∫∑c ƒë·ªãnh khi expose Service l√† route/v1

C√∫ ph√°p ƒë·∫ßy ƒë·ªß nh·∫•t (hay d√πng):
```
oc expose service <service-name> \
  --name=<route-name> \
  --port=<service-port> \
  --hostname=<custom-host> \
  --path=<url-path>
```
üîπ `--hostname=<custom-host>`

- D√πng khi b·∫°n mu·ªën Route c√≥ hostname c·ª• th·ªÉ thay v√¨ hostname ng·∫´u nhi√™n OpenShift generate.
- Tr∆∞·ªùng h·ª£p d√πng:
  - B·∫°n mu·ªën public domain d·ªÖ nh·ªõ, v√≠ d·ª•:
```
oc expose service myservice --hostname=weather.apps.ocp.example.com
```
- Khi b·∫°n ƒë√£ c√≥ DNS record tr·ªè v·ªÅ OpenShift router (Ingress).
- Trong m√¥i tr∆∞·ªùng production, hostname th∆∞·ªùng ƒë∆∞·ª£c quy ƒë·ªãnh tr∆∞·ªõc (VD: api.company.com, shop.company.com).

üëâ N·∫øu kh√¥ng set, OpenShift s·∫Ω t·ª± t·∫°o t√™n d·∫°ng:
```
<route-name>-<project>.apps.<cluster-domain>
```
üîπ`--path=<url-path>`
- D√πng ƒë·ªÉ th√™m context path cho route.
- Tr∆∞·ªùng h·ª£p d√πng:
  - Khi b·∫°n mu·ªën nhi·ªÅu ·ª©ng d·ª•ng share chung 1 hostname, nh∆∞ng ph√¢n bi·ªát b·∫±ng path.
V√≠ d·ª•:
```
oc expose service backend --hostname=api.apps.ocp.example.com --path=/backend
oc expose service frontend --hostname=api.apps.ocp.example.com --path=/frontend
```
- Khi ·ª©ng d·ª•ng c·ªßa b·∫°n l·∫Øng nghe tr√™n root /, nh∆∞ng b·∫°n ch·ªâ mu·ªën expose d∆∞·ªõi m·ªôt nh√°nh /app1 ch·∫≥ng h·∫°n.

üëâ N·∫øu kh√¥ng set, m·∫∑c ƒë·ªãnh path l√† `/` (root).

- `--hostname` ‚Üí d√πng khi b·∫°n mu·ªën route c√≥ domain c·ª• th·ªÉ, nh·∫•t l√† trong production (g·∫Øn DNS chu·∫©n).

- `--path` ‚Üí d√πng khi mu·ªën ch·∫°y nhi·ªÅu service/app d∆∞·ªõi c√πng 1 domain nh∆∞ng ph√¢n bi·ªát b·∫±ng ƒë∆∞·ªùng d·∫´n.


OpenShift ch·ªçn port n√†o ƒë·ªÉ expose khi dung `oc expose` command ? 
- oc expose service ... m·∫∑c ƒë·ªãnh l·∫•y port t·ª´ Service.
- N·∫øu Service c√≥ 1 port ‚Üí Route d√πng port ƒë√≥.
- N·∫øu Service c√≥ nhi·ªÅu port ‚Üí b·∫°n ph·∫£i ch·ªâ ƒë·ªãnh --port.

*C√°ch ki·ªÉm tra port service*
```
oc get service openshift-dev-deploy-cli-weather -o yaml
---
ports:
- name: 8080-tcp
  port: 8080
  targetPort: 8080

```
Service c√≥ 8080 v√† 8443 ‚Üí ph·∫£i ch·ªâ ƒë·ªãnh:
```
oc expose service myservice --port=8080
```

**Inner loop vs Outer loop**

![alt text](pic/7.png)

1. Inner loop vs Outer loop

- Inner loop: v√≤ng l·∫∑p ng·∫Øn c·ªßa dev ‚Üí code ‚Üí build ‚Üí test nhanh ‚Üí debug ‚Üí l·∫∑p l·∫°i.
  - ƒê√¢y l√† ph·∫ßn m√† odo h·ªó tr·ª£ m·∫°nh nh·∫•t (deploy code nhanh l√™n cluster ƒë·ªÉ test).

- Outer loop: v√≤ng l·∫∑p d√†i h∆°n ‚Üí build artifact ch√≠nh th·ª©c ‚Üí ch·∫°y integration test, security test ‚Üí deploy v√†o m√¥i tr∆∞·ªùng prod/staging.
  - odo c≈©ng c√≥ th·ªÉ ƒë·ªãnh nghƒ©a ƒë∆∞·ª£c qua devfile.yaml.

2. Devfile (devfile.yaml)

L√† file trung t√¢m, m√¥ t·∫£:
- Components: container images, manifests, volume‚Ä¶
- Commands: c√°c b∆∞·ªõc c·∫ßn ch·∫°y (build, run, test, debug, deploy).
- Command groups: gom l·ªánh theo nh√≥m (`build, run, test, debug, deploy`).
Nh·ªù devfile, odo bi·∫øt ph·∫£i l√†m g√¨ khi b·∫°n g√µ:
- odo build
- odo run
- odo test
- odo debug
- odo deploy

Note: 
- N·∫øu mu·ªën odo <kind> ch·∫°y m·∫∑c ƒë·ªãnh, b·∫°n ph·∫£i b·∫≠t isDefault: true.
- N·∫øu c√≥ nhi·ªÅu command c√πng kind th√¨ ch·ªâ 1 ƒë∆∞·ª£c default, c√≤n l·∫°i ph·∫£i g·ªçi b·∫±ng --command <id>.

3. `odo init`
- T·∫°o file devfile.yaml ban ƒë·∫ßu.
- odo s·∫Ω c·ªë g·∫Øng ƒëo√°n runtime t·ª´ source code (v√≠ d·ª• th·∫•y package.json ‚Üí Node.js, th·∫•y requirements.txt ‚Üí Python).
- N·∫øu b·∫°n kh√¥ng truy·ªÅn options, n√≥ s·∫Ω m·ªü interactive mode ƒë·ªÉ b·∫°n ch·ªçn.
- N√≥ c·∫ßn k·∫øt n·ªëi t·ªõi devfile registry (m·∫∑c ƒë·ªãnh: https://registry.devfile.io
) ƒë·ªÉ l·∫•y template ph√π h·ª£p.

> Sau khi c√≥ devfile.yaml, b·∫°n c√≥ th·ªÉ s·ª≠a ƒë·ªÉ ph√π h·ª£p v·ªõi project.

4. L·ªánh `odo create project`
```
odo create project PROJECT_NAME
```
- T·∫°o m·ªôt OpenShift Project (namespace) m·ªõi tr·ª±c ti·∫øp t·ª´ odo.
- T∆∞∆°ng ƒë∆∞∆°ng v·ªõi l·ªánh: `oc new-project PROJECT_NAME`

The following devfile provides a simplified example for outer loop development with odo. The devfile adheres to the following requirements:

- The `Dockerfile` must exist in the same directory as the `devfile.yaml`.

- The `deploy.yaml` contains the Kubernetes resources for the application.

- The parent devfile must exist.

- The odo command must be able to access Podman and an authenticated image registry.

![alt text](pic/10.png)

Example deploy.yaml
```bash
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nodejs-app
  labels:
    app: nodejs-app
spec:
...
---
apiVersion: v1
kind: Service
metadata:
  name: nodejs-app
  labels:
    app: nodejs-app
spec:
  selector:

...
---
apiVersion: route.openshift.io/v1
kind: Route
metadata:
  name: nodejs-app
spec:
  to:

```
**Container Image Renaming with odo**  
*V·∫•n ƒë·ªÅ*
- Trong devfile.yaml b·∫°n c√≥ th·ªÉ ƒë·ªãnh nghƒ©a image component ƒë·ªÉ build/push image.
- N·∫øu b·∫°n vi·∫øt th·∫≥ng image name ƒë·∫ßy ƒë·ªß (v√≠ d·ª•: quay.io/myuser/nodejs-app:1.0), th√¨ devfile b·ªã g·∫Øn ch·∫∑t v·ªõi registry ƒë√≥ ‚Üí kh√≥ chia s·∫ª cho ng∆∞·ªùi kh√°c.
- V√¨ v·∫≠y, odo h·ªó tr·ª£ image renaming ƒë·ªÉ l√†m devfile portable (d√πng l·∫°i ·ªü nhi·ªÅu cluster/registry kh√°c nhau).

*C√°ch ho·∫°t ƒë·ªông*

1. Khai b√°o `ImageRegistry`
B·∫°n set registry m·∫∑c ƒë·ªãnh ƒë·ªÉ odo push image:
```
odo preference set ImageRegistry quay.io/myuser

# syntax
[user@host ~]$ odo preference set ImageRegistry REGISTRY_URL/NAMESPACE
```
‚Üí Sau ƒë√≥ m·ªçi image s·∫Ω ƒë∆∞·ª£c push v·ªÅ quay.io/myuser/....

2. D√πng imageName t∆∞∆°ng ƒë·ªëi trong devfile  
Trong `devfile.yaml`:
```
components:
  - name: relative-image
    image:
      imageName: "my-relative-image"   # ch·ªâ c√≥ t√™n, kh√¥ng c√≥ registry
```

üëâ ƒê√¢y g·ªçi l√† relative image name.

3. Khi build/push, odo s·∫Ω t·ª± ƒë·ªïi t√™n image th√†nh:
```
<ImageRegistry>/<DevfileName>-<ImageName>:<UniqueId>
```
- ImageRegistry: gi√° tr·ªã b·∫°n set (quay.io/myuser)
- DevfileName: t√™n devfile (v√≠ d·ª• nodejs-app)
- ImageName: ph·∫ßn b·∫°n ƒë·ªãnh nghƒ©a (my-relative-image)
- UniqueId: gi√° tr·ªã random ƒë·ªÉ tr√°nh tr√πng

üìù V√≠ d·ª• c·ª• th·ªÉ

Devfile nodejs-app c√≥:
```
components:
  - name: relative-image
    image:
      imageName: "my-relative-image"
```

B·∫°n set:
```
odo preference set ImageRegistry quay.io/nghia
```

Khi odo deploy, image th·ª±c t·∫ø ƒë∆∞·ª£c push s·∫Ω th√†nh:
```
quay.io/nghia/nodejs-app-my-relative-image:abc123
```
‚úÖ L·ª£i √≠ch

- Devfile kh√¥ng b·ªã hard-code registry ‚Üí portable, d·ªÖ chia s·∫ª cho team kh√°c.
- odo t·ª± ƒë·ªông ƒë·ªïi t√™n v√† push image ƒë√∫ng registry b·∫°n ch·ªâ ƒë·ªãnh.
- B·∫°n c√≥ th·ªÉ ki·ªÉm tra:
  - Trong registry (quay.io/nghia/...)
  - Ho·∫∑c trong resource YAML m√† odo apply l√™n cluster.

**Syntax h·ªØu √≠ch c·ªßa oc new-app**
```bash
oc new-app <image>                  # T·∫°o t·ª´ image
oc new-app --image=<image>
oc new-app <image>~<git-repo>       # S2I build t·ª´ source code
oc new-app -f template.yaml         # D√πng template file
oc new-app --name=myapp             # ƒê·∫∑t t√™n app
oc new-app <image> -e VAR=VALUE     # Th√™m bi·∫øn m√¥i tr∆∞·ªùng
oc new-app <image> --as-deployment-config  # D√πng DeploymentConfig thay v√¨ Deployment
```

**Image m√† oc new-app d√πng t·ª´ ƒë√¢u ra?**

![alt text](pic/11.png)

üîπ 2. V·∫≠y podman build li√™n quan g√¨?
- N·∫øu b·∫°n t·ª± build m·ªôt image b·∫±ng podman build tr√™n m√°y local th√¨ image ƒë√≥ ch·ªâ n·∫±m trong local host.
- ƒê·ªÉ OpenShift d√πng ƒë∆∞·ª£c, b·∫°n ph·∫£i push image ƒë√≥ l√™n m·ªôt registry (v√≠ d·ª•: quay.io, docker.io, ho·∫∑c internal OpenShift registry image-registry.openshift-image-registry.svc:5000).

Sau ƒë√≥ b·∫°n m·ªõi d√πng:
```
oc new-app <registry>/<namespace>/<image>:<tag>
```

M·∫∑c ƒë·ªãnh `oc new-app` ch·ªâ gi√∫p b·∫°n deploy nhanh ƒë·ªÉ test, c√≤n n·∫øu mu·ªën t√πy ch·ªânh chi ti·∫øt th√¨ c√≥ v√†i c√°ch:

![alt text](pic/12.png)

**T√πy ch·ªânh**

- T√πy ch·ªânh ngay t·ª´ l·ªánh `oc new-app`
```bash
# Example
oc scale deployment myapp --replicas=3
oc expose deployment myapp --port=8080 --target-port=8080
```
- Ch·ªânh sau khi resource ƒë∆∞·ª£c t·∫°o
```bash
# Xu·∫•t YAML ra file ƒë·ªÉ ch·ªânh:
oc get deployment myapp -o yaml > myapp-deploy.yaml

# -> S·ª≠a YAML (replicas, resource limits, volume, env, liveness probe‚Ä¶)

# Apply l·∫°i:
oc apply -f myapp-deploy.yaml
```
3. D√πng workflow hi·ªán ƒë·∫°i (recommended)  
Thay v√¨ `oc new-app` (quick & simple), b·∫°n c√≥ th·ªÉ:
- Vi·∫øt s·∫µn Deployment + Service YAML ‚Üí `oc apply -f`.
- Ho·∫∑c d√πng odo v·ªõi devfile.yaml n·∫øu ph√°t tri·ªÉn app.
- Ho·∫∑c d√πng BuildConfig + ImageStream n·∫øu mu·ªën workflow CI/CD theo ki·ªÉu OpenShift truy·ªÅn th·ªëng.

---
# Chapter 3.  Building and Publishing Container Images

Red Hat Universal Base Images
When defining custom container images, Red Hat recommends the use of Red Hat Universal Base Images (UBI) as the base container images for your applications. UBI images are certified, tested, and regularly maintained images that Red Hat provides at no cost.

UBI images also provide the following major benefits:

**Universal**  
UBI images are designed to be used as the base images for developing container-based applications.

**Robust**  
UBI images are based on Red Hat Enterprise Linux (RHEL). This brings characteristics such as stability and vulnerability management to your base container images.

**Standard**  
UBI images are compliant with the Open Container Initiative (OCI).

**Extensible**  
UBI images provide package managers and other tools for installing additional software.

**OpenShift-optimized**  
UBI images are tailored to work well on Red Hat OpenShift.

**Redistributable**  
The UBI End-User Licensing Agreement (EULA) permits free distribution of the applications that you build on top of UBI images.

Red Hat provides four types of UBI images, designed to cover most use cases.

Image type	|Image name	|Uses
---|---|---
Standard	|ubi|	For most applications and use cases
Init	|ubi-init|	For containers that run multiple systemd services
Minimal	|ubi-minimal	|Smaller image for applications that manage their own dependencies and depend on fewer OS components
Micro	|ubi-micro	|Smallest image for optimized memory-footprint use cases; for applications that use almost no OS components

Runtime UBI Images For Developers
Red Hat provides UBI images for the following runtime languages:
- OpenJDK
- Node.js
- Python
- PHP
- .NET
- Go
- Ruby
 
Optimize Containerfiles for OpenShift  
Format
```
registry.access.redhat.com/NAMESPACE/NAME[:TAG]
```
Example 
```
FROM registry.access.redhat.com/ubi10/nodejs-22-minimal:10.0
```
**Ensure That Your Containers Handle Interruption Signals**

üîπ 1. C∆° ch·∫ø shutdown m·∫∑c ƒë·ªãnh

Khi b·∫°n x√≥a Pod ho·∫∑c rollout Deployment m·ªõi, OpenShift s·∫Ω:
- G·ª≠i t√≠n hi·ªáu SIGTERM ƒë·∫øn process PID 1 trong container.
- Container/app c·ªßa b·∫°n c√≥ tr√°ch nhi·ªám ng·∫Øt k·∫øt n·ªëi, ƒë√≥ng resource, l∆∞u data‚Ä¶.
- N·∫øu trong th·ªùi gian terminationGracePeriodSeconds (m·∫∑c ƒë·ªãnh 30s) app kh√¥ng t·∫Øt ‚Üí OpenShift g·ª≠i SIGKILL (kill ngay l·∫≠p t·ª©c).

üëâ Do ƒë√≥, ·ª©ng d·ª•ng ph·∫£i bi·∫øt c√°ch handle SIGTERM ƒë·ªÉ shutdown ‚Äú√™m ƒë·∫πp‚Äù (graceful shutdown).

üîπ 2. Tr∆∞·ªùng h·ª£p ·ª©ng d·ª•ng kh√¥ng handle SIGTERM

V√≠ d·ª•:
- M·ªôt app Java ch·∫°y b·∫±ng java -jar example.jar.
- N·∫øu b·∫°n kh√¥ng trap SIGTERM, khi Pod b·ªã kill ‚Üí Java process s·∫Ω t·∫Øt ngay ‚Üí c√≥ th·ªÉ m·∫•t data, ch∆∞a commit transaction, connection b·ªã c·∫Øt ƒë·ªôt ng·ªôt.

C√°ch x·ª≠ l√Ω:
- Vi·∫øt entrypoint script (nh∆∞ v√≠ d·ª• trong b√†i): trap t√≠n hi·ªáu SIGTERM v√† forward cho app, r·ªìi wait cho app shutdown.
```
trap graceful_shutdown SIGTERM
java -jar example.jar &
java_pid=$!
wait "$java_pid"
```

·ªû ƒë√¢y:

- trap graceful_shutdown SIGTERM: khi nh·∫≠n SIGTERM ‚Üí g·ªçi h√†m graceful_shutdown.

- H√†m n√†y g·ª≠i SIGTERM ƒë·∫øn process Java (kill -SIGTERM $java_pid) v√† ƒë·ª£i n√≥ shutdown.

üîπ 3. Khi app kh√¥ng s·ª≠a code ƒë∆∞·ª£c

N·∫øu ·ª©ng d·ª•ng kh√¥ng c√≥ c∆° ch·∫ø nh·∫≠n SIGTERM (ho·∫∑c b·∫°n kh√¥ng th·ªÉ thay ƒë·ªïi entrypoint), b·∫°n c√≥ th·ªÉ d√πng Pod lifecycle hook:
```
lifecycle:
  preStop:
    httpGet:
      path: /shutdown
      port: 8080
```

- Khi Pod chu·∫©n b·ªã b·ªã x√≥a, kubelet s·∫Ω g·ªçi HTTP GET v√†o /shutdown.

- ·ª®ng d·ª•ng s·∫Ω nh·∫≠n request n√†y v√† th·ª±c hi·ªán cleanup (ƒë√≥ng k·∫øt n·ªëi, flush cache, l∆∞u tr·∫°ng th√°i, v.v).

- Sau ƒë√≥ m·ªõi nh·∫≠n SIGTERM ƒë·ªÉ t·∫Øt h·∫≥n.

üîπ 4. N·∫øu app v·∫´n kh√¥ng t·∫Øt?

- Sau khi h·∫øt th·ªùi gian terminationGracePeriodSeconds, kubelet/OpenShift g·ª≠i SIGKILL ‚Üí process b·ªã kill ngay l·∫≠p t·ª©c.

- L√∫c n√†y kh√¥ng c√≥ c∆° h·ªôi cleanup ‚Üí nguy c∆° m·∫•t d·ªØ li·ªáu ho·∫∑c l·ªói.

‚úÖ T√≥m l·∫°i

- OpenShift/K8s lu√¥n g·ª≠i SIGTERM tr∆∞·ªõc ƒë·ªÉ cho app t·ª± shutdown √™m ƒë·∫πp.

- App n√™n handle SIGTERM (qua entrypoint script).

- N·∫øu kh√¥ng th·ªÉ ‚Üí d√πng preStop hook ƒë·ªÉ app cleanup tr∆∞·ªõc khi b·ªã kill.

- N·∫øu app v·∫´n kh√¥ng shutdown k·ªãp ‚Üí cu·ªëi c√πng b·ªã SIGKILL (force kill).

![alt text](pic/13.png)

**Reduce Image Size**

1. Gi·∫£m s·ªë l∆∞·ª£ng RUN
- K·∫øt h·ª£p nhi·ªÅu l·ªánh th√†nh 1 RUN ƒë·ªÉ gi·∫£m s·ªë layer.

2. Gi·∫£m build context
- D√πng .dockerignore / .containerignore ƒë·ªÉ lo·∫°i b·ªè file/th∆∞ m·ª•c kh√¥ng c·∫ßn thi·∫øt.

3. Multistage build
- Stage ƒë·∫ßu: build app b·∫±ng image ƒë·∫ßy ƒë·ªß (v√≠ d·ª•: ubi9/nodejs-22).
- Stage cu·ªëi: copy artifact sang image minimal/runtime (ubi9/nodejs-22-minimal).
- ‚Üí K·∫øt qu·∫£: image nh·ªè g·ªçn, ch·ªâ ch·ª©a runtime + app.

4. D√πng minimal image

- Node.js: nodejs-22-minimal.
- OpenJDK: ubi9/openjdk-21-runtime.

5. LABEL
- D√πng LABEL ƒë·ªÉ khai b√°o metadata.
- V√≠ d·ª•: LABEL io.openshift.min-cpu 2 ‚Üí UI OpenShift c·∫£nh b√°o c·∫ßn √≠t nh·∫•t 2 CPU.

6. WORKDIR
- Lu√¥n d√πng WORKDIR v·ªõi absolute path thay v√¨ nhi·ªÅu l·∫ßn cd trong RUN.
7. ENV v√† ARG
- D√πng ENV ƒë·ªÉ c·∫•u h√¨nh ƒë∆∞·ªùng d·∫´n, version, PATH‚Ä¶
- D√πng ARG cho bi·∫øn build-time ‚Üí t·∫°o image t√°i s·ª≠ d·ª•ng ƒë∆∞·ª£c.
8. VOLUME
- Khai b√°o r√µ VOLUME ƒë·ªÉ ng∆∞·ªùi d√πng bi·∫øt mount d·ªØ li·ªáu.
- N·∫øu kh√¥ng mount, OpenShift s·∫Ω t·ª± g·∫Øn EmptyDir (ephemeral).

9. EXPOSE
- Ch·ªâ expose port >1024 (kh√¥ng c·∫ßn quy·ªÅn root).
- oc new-app s·∫Ω t·ª± t·∫°o Deployment + Service theo port ƒë∆∞·ª£c EXPOSE.
- Web console c≈©ng nh·∫≠n di·ªán c√°c port n√†y ƒë·ªÉ c·∫•u h√¨nh service.

üëâ T√≥m g·ªçn l·∫°i:
- Multistage + minimal UBI image ‚Üí gi·∫£m size.
- LABEL, ENV, VOLUME, EXPOSE ‚Üí gi√∫p OpenShift hi·ªÉu r√µ image, h·ªó tr·ª£ c·∫•u h√¨nh d·ªÖ d√†ng.
- WORKDIR v√† √≠t RUN ‚Üí l√†m Containerfile g·ªçn, d·ªÖ maintain.

**Build and Push Images with Podman**  
You can use a tool such as Podman to build a container image locally and push the image to a container registry.

> Note
You can also use OpenShift to build your container images. The OpenShift build capabilities, such as Source-to-Image (S2I) and Docker builds, are covered later in this course. OpenShift also provides the OpenShift Builds framework, which is based on the Shipwright project.

Use the podman build command to create a container image from a Containerfile, as the following example shows:
```
[user@host ~]$ podman build CONTEXT_DIR -t IMAGE
```
The preceding command creates a local container image by using the Containerfile or Dockerfile at the CONTEXT_DIR directory. The produced image is called IMAGE. After building the container image locally, push the image to a container registry by using the podman push command.
```
[user@host ~]$ podman push IMAGE
```
> Note
You must be logged in to the registry to push images. You can log in with the podman login command.

After the image is published in the container registry, you can deploy the image by using any of the methods that Red Hat OpenShift provides, such as the web console, or the oc and odo CLIs.

## Guided Exercise: Building Container Images for Red Hat OpenShift


```bash
oc new-app \
--name greetings \
--image=registry.ocp4.example.com:8443/developer/images-ubi-greetings:1.0.0
```
File Containerfile
![alt text](pic/15.png)

3 error khi `oc new-app`  
![alt text](pic/14.png)

**Fix 1**  
Remove the USER instruction from the Containerfile. The file must look as follows:
```
FROM registry.ocp4.example.com:8443/ubi10/nodejs-22-minimal:10.0

ENV PORT=80
EXPOSE ${PORT}

ADD . $HOME

RUN npm ci --omit=dev && rm -rf .npm

CMD npm start
```

**Fix 2**  
Trong Linux (v√† c≈©ng ƒë√∫ng trong container), c√≥ quy t·∫Øc:
- Port < 1024 = privileged ports ‚Üí ch·ªâ process ch·∫°y b·∫±ng root user m·ªõi bind ƒë∆∞·ª£c.
- Port ‚â• 1024 = unprivileged ports ‚Üí b·∫•t k·ª≥ non-root user n√†o c≈©ng bind ƒë∆∞·ª£c.

V√¨ sao l·ªói x·∫£y ra?
- ·ª®ng d·ª•ng c·ªßa b·∫°n c·ªë g·∫Øng l·∫Øng nghe tr√™n c·ªïng 80:
```
Error: listen EACCES: permission denied 0.0.0.0:80
```
- Nh∆∞ng trong OpenShift, container ch·∫°y v·ªõi m·ªôt UID ng·∫´u nhi√™n, kh√¥ng ph·∫£i root.
- Do ƒë√≥, process ƒë√≥ kh√¥ng th·ªÉ m·ªü ƒë∆∞·ª£c c·ªïng 80 ‚Üí b√°o l·ªói EACCES.

**Fix 3**  
Fix the permissions of the /var/cache directory. In the Containerfile, add a RUN instruction that runs as the root user and ensures that the group assigned to the /var/cache directory is the root group (0). Then, it should grant the root group the same permissions as the user that owns this directory. Finally, it should restore 1001 as the user ID that runs the application.
```bash
...omitted...
RUN npm ci --omit=dev && rm -rf .npm

USER root
RUN chgrp -R 0 /var/cache && \
    chmod -R g=u /var/cache
USER 1001

CMD npm start
```
-> rebuild image, run -rm , podman push -> oc new-app -> expose > get url

## Using External Registries in Red Hat OpenShift
There are many kinds of container registries:

**Public registries**  
Registries that allow anyone to consume container images directly from the internet without any authentication, such as Docker Hub, Quay.io, or the Red Hat Registry.

**Private registries**  
Registries that are available only to selected consumers and usually require authentication. The Red Hat terms-based registry is an example of a private container registry.

**Enterprise registries**  
Registries that your organization manages. Such registries are usually available only to the organization's employees.

**OpenShift internal registries**  
A registry server managed internally by an OpenShift cluster to store container images.

These kinds of registries are not mutually exclusive: a registry can be, at the same time, both public and private.

**Creating Registry Credentials in OpenShift**  
1. You can use the oc create command to create a secret, for example:
```
[user@host ~]$ oc create secret generic example-secret \
--from-literal=user=developer --namespace=example-ns
secret/example-secret created
```
2. You can use the OpenShift console to create secrets. In the Developer perspective, click Secrets. Select a project, click Create, and then select the secret type that you want to create.

![alt text](pic/17.png)

3. Kubernetes provides the docker-registry secret type to store credentials for authentication with the container registry.

```bash
[user@host ~]$ oc create secret docker-registry SECRET_NAME \
--docker-server REGISTRY_URL \
--docker-username USER \
--docker-password PASSWORD \
--docker-email=EMAIL
secret/SECRET_NAME created
```
4. T·∫°o tr·ª±c ti·∫øp trong Web Console (GUI)
- V√†o Workloads ‚Üí Secrets ‚Üí Create ‚Üí Image pull secret.
- Nh·∫≠p server URL, user, password, email.

![alt text](pic/16.png)

You can also create the secret from existing credentials. For example, if you logged in to the private registry with Podman, then you have existing credentials in the `${XDG_RUNTIME_DIR}/containers/auth.json` file. Because the auth.json file uses the same structure as the .dockerconfigjson file, you can create the secret by using the `auth.json` file.
```
[user@host ~]$ oc create secret generic SECRET_NAME \
--from-file .dockerconfigjson=${XDG_RUNTIME_DIR}/containers/auth.json \
--type kubernetes.io/dockerconfigjson
```
You can also upload the auth.json file in the OpenShift console when creating the secret.

**Configuring OpenShift to Use the Registry Credentials**  
You can configure OpenShift to use custom credentials by using the `spec.imagePullSecrets` Pod property, for example:
```
apiVersion: v1
kind: Pod
metadata:
  name: example-pod
spec:
  containers:
  - name: example-container
    image: REGISTRY_URL
  imagePullSecrets:
  - name: SECRET_NAME
```
Consequently, you can use the property for controllers, such as the Deployment objects:
```
apiVersion: apps/v1
kind: Deployment
metadata:
  name: example-deployment
spec:
  replicas: 3
  selector:
    matchLabels:
      app: my-app
  template:
    metadata:
      labels:
        app: my-app
    spec:
      containers:
        - name: example-container
          image: REGISTRY_URL
      imagePullSecrets:
        - name: SECRET_NAME
```

**Linking Registry Credentials to Service Accounts**

Instead of manually assigning the credentials to pods, you can configure OpenShift to assign the credentials to pods automatically by using service accounts. A service account provides an identity for pods. Pods use the default service account unless you configure a different service account.

Use the oc secrets link command to connect a secret with a service account, for example:
```
[user@host ~]$ oc secrets link --for=pull default SECRET_NAME
no output expected
```
Ho·∫∑c cho build config:
```
oc secrets link builder my-registry-secret --for=pull
```
The preceding command creates a new entry in the service account imagePullSecrets field:
```
apiVersion: v1
kind: ServiceAccount
metadata:
  name: default
imagePullSecrets:
- name: SECRET_NAME
```
When you create a pod that uses the default service account, it inherits the imagePullSecrets field without you explicitly specifying the field in the pod definition.

This means that every pod that uses the default service account is authorized with the registry credentials in your secret.  
Giai thich:
- B·∫°n ƒë√£ t·∫°o secret tr∆∞·ªõc ƒë√≥ (ch·ª©a th√¥ng tin registry account).
- C√¢u l·ªánh n√†y s·∫Ω g·∫Øn secret v√†o ServiceAccount default c·ªßa project hi·ªán t·∫°i.
- K·ªÉ t·ª´ l√∫c ƒë√≥, b·∫•t k·ª≥ Pod n√†o trong project ch·∫°y v·ªõi SA default s·∫Ω t·ª± ƒë·ªông d√πng secret n√†y khi pull image t·ª´ registry private.

-> 2 c√°ch ch√≠nh ƒë·ªÉ s·ª≠ d·ª•ng secret ƒë√≥ trong OpenShift/Kubernetes:
- C√°ch 1: Khai b√°o tr·ª±c ti·∫øp trong workload (spec.imagePullSecrets)
- C√°ch 2: G·∫Øn secret v√†o ServiceAccount (oc secrets link)

Nguy√™n t·∫Øc
- Secret l√† resource ‚Äúnamespace-scoped‚Äù ‚Üí ch·ªâ t·ªìn t·∫°i trong project (namespace) n√†o n√≥ ƒë∆∞·ª£c t·∫°o.
- Do ƒë√≥:
  - Secret trong project A kh√¥ng th·ªÉ d√πng tr·ª±c ti·∫øp ·ªü project B.
  - N·∫øu project B c≈©ng c·∫ßn d√πng ‚Üí b·∫°n ph·∫£i t·∫°o l·∫°i secret trong project B (c√≥ th·ªÉ c√πng th√¥ng tin).

C√≥ th·ªÉ dung ServiceAccount kh√°c thay v√¨ default
```
oc secrets link custom-sa my-registry-secret --for=pull

# 
apiVersion: apps/v1
kind: Deployment
metadata:
  name: myapp
spec:
  replicas: 1
  selector:
    matchLabels:
      app: myapp
  template:
    metadata:
      labels:
        app: myapp
    spec:
      serviceAccountName: custom-sa   # d√πng SA n√†y thay v√¨ default
      containers:
      - name: myapp
        image: quay.io/myuser/private-app:1.0

```

*Search for the failed event messages that the application emits.*
```
[student@workstation ~]$ oc get event --field-selector type=Warning \
-o jsonpath='{range .items[]}{.message}{"\n"}{end}'
Failed to pull image "registry.ocp4.example.com:8443/redhattraining/hello-world-nginx:latest": ... invalid username/password: unauthorized: ...
```

![alt text](pic/18.png)

Docs: https://docs.redhat.com/en/documentation/openshift_container_platform/4.8/html/authentication_and_authorization/understanding-and-creating-service-accounts


Fix error  "Robot Accounts"

![alt text](pic/19.png)

7. Delete the robot account from the internal registry.

On the Robot Accounts page, click the gear icon of the developer+ocprobot account, and then click Delete Robot developer+ocprobot. Click Ok to complete the process.

## 3.5 Creating Image Streams
Image streams have the following benefits:
- They provide a level of indirection to the container image that OpenShift runs.
- They allow for rolling back to a previous container version without updating the image registry.
- They enable build and deployment automations when an image stream tag gets updated.
- They enable the caching of images from external image registries.
- You can use role-based access control (RBAC) on the image stream object to secure access to container images.

V·∫•n ƒë·ªÅ: m·ªói l·∫ßn ƒë·ªïi image ‚Üí b·∫°n ph·∫£i can thi·ªáp th·ªß c√¥ng.
Trong khi ƒë√≥, n·∫øu d√πng ImageStream + ImageChange trigger th√¨ ch·ªâ c·∫ßn c·∫≠p nh·∫≠t oc tag ho·∫∑c build m·ªõi   
‚Üí OpenShift t·ª± rollout cho b·∫°n.


![alt text](pic/20.png)

![alt text](pic/21.png)

**Managing Image Streams and Tags**

For example, the following command imports a my-app-stream container image from an external container registry and periodically checks for updates:
```
[user@host ~]$ oc import-image myimagestream --confirm --scheduled=true \
--from example.com/example-repo/my-app-image
```
To create one image stream tag resource for each container image tag that exists in the source registry server, add the --all option to the oc import-image command. The following command creates or updates all image stream tags for new tags on the source registry server:
```
[user@host ~]$ oc import-image myimagestream --confirm --all \
--from registry/myorg/myimage
```
Running the oc import-image command on an existing image stream updates one of its current image stream tags to the current image IDs on the source registry server, such as in the following command:
```
[user@host ~]$ oc import-image myimagestream[:tag]
```
Exert finer control over an image stream tag by using the oc tag command. This enables you to associate an image stream tag with the following:
- A different registry than the one in its image stream
- A different container image name and tag
- An image ID that might not be the one currently associated with the image tag on the registry server
- An alias for the image stream tag

For example, to update the latest image stream tag to point to a different tag you can run the following command:
```
[user@host ~]$ oc tag myimagestream:tag myimagestream:latest
```
**Creating Image Streams From Private Registries**  
The following example commands use Podman to log in to a private registry, create a secret to store the access token, and create an image stream that points to the private registry:
```
[user@host ~]$ podman login -u myuser registry.example.com
[user@host ~]$ oc create secret generic regtoken \
--from-file .dockerconfigjson=${XDG_RUNTIME_DIR}/containers/auth.json \
--type kubernetes.io/dockerconfigjson
[user@host ~]$ oc import-image myimagestream --confirm \
--from registry.example.com/myorg/myimage
```
After you create an image stream, you can use it to deploy an application by using the oc new-app command and by using the `-i` option to specify the image stream.

By default, an image stream resource is only available to create applications or builds in the same project.

**Using Image Streams with Kubernetes Resources**

**Sharing an Image Stream Between Multiple Projects**

üìå 1. T·∫°o v√† qu·∫£n l√Ω ImageStream

T·∫°o IS r·ªóng:
```
oc create is myapp
```

Xem danh s√°ch IS trong project:
```
oc get is
```

Xem chi ti·∫øt 1 IS:
```
oc describe is myapp
```

Xo√° IS:
```
oc delete is myapp
```
üìå 2. Import image t·ª´ registry v√†o IS

Import m·ªôt image c·ª• th·ªÉ:
```
oc import-image myapp:1.0 --from=quay.io/example/myapp:1.0 --confirm
```

Import t·∫•t c·∫£ tag t·ª´ m·ªôt repo:
```
oc import-image myapp --from=quay.io/example/myapp --all --confirm
```

Import c√≥ l·ªãch tr√¨nh (scheduled):
```
oc import-image myapp --from=quay.io/example/myapp --confirm --scheduled=true
```

C·∫≠p nh·∫≠t tag hi·ªán c√≥:
```
oc import-image myapp:latest
```
üìå 3. Qu·∫£n l√Ω tag c·ªßa IS

T·∫°o alias gi·ªØa c√°c tag:
```
oc tag myapp:1.0 myapp:latest
```

Copy t·ª´ IS kh√°c:
```
oc tag otheris:2.0 myapp:dev
```

G√°n IS tag v·ªõi image ngo√†i registry:
```
oc tag quay.io/example/myapp:3.0 myapp:stable
```
üìå 4. Li√™n quan t·ªõi ·ª©ng d·ª•ng

D√πng IS ƒë·ªÉ t·∫°o app (DeploymentConfig):
```
oc new-app -i myapp:latest --name=myapp
```

Xem Pod n√†o ƒëang d√πng IS:
```
oc describe is myapp
```

(ph·∫ßn ‚ÄúImage Stream Tag History‚Äù s·∫Ω show DeploymentConfig, Builds n√†o ƒëang d√πng tag ƒë√≥).

üìå 5. BuildConfig v√† IS

Build output v·ªÅ IS:
```
oc new-build --binary --name=myapp --to=myapp:1.0
oc start-build myapp --from-dir=. --follow
```

üëâ T√≥m g·ªçn:

- oc create is / oc delete is ‚Üí qu·∫£n l√Ω ImageStream.
- oc import-image ‚Üí k√©o image t·ª´ registry v√†o IS.
- oc tag ‚Üí qu·∫£n l√Ω tag trong IS.
- oc new-app -i ‚Üí deploy app t·ª´ IS (DeploymentConfig + trigger).
- oc describe is ‚Üí theo d√µi ai ƒëang d√πng IS.

---
# Chapter 4.  Managing Red Hat OpenShift Builds
üîπ OpenShift Build Process (C√°ch OpenShift build image)
1. C√°c th√†nh ph·∫ßn ch√≠nh khi build

- Trigger ‚Üí c√°i g√¨ kh·ªüi ch·∫°y build (git commit, webhook, thay ƒë·ªïi image).
- Strategy ‚Üí build theo c√°ch n√†o (Source-to-Image (S2I), Dockerfile/Buildah, Custom).
- Input sources ‚Üí code, binary, ho·∫∑c Dockerfile.
- Output ‚Üí image ƒë∆∞·ª£c push v√†o registry.

üëâ OpenShift cung c·∫•p 2 c√°ch build ch√≠nh:

- BuildConfig (c·ªï ƒëi·ªÉn): t√†i nguy√™n g·ªëc c·ªßa OpenShift, khai b√°o YAML r·ªìi build.
- Shipwright (m·ªõi, Kubernetes-native): d·ª±a tr√™n upstream project, linh ho·∫°t, h·ªó tr·ª£ nhi·ªÅu tool (S2I, Buildah, Buildpacks).

2. Shipwright (Builds for OpenShift)
- Kubernetes-native: d√πng CRD nh∆∞ c√°c resource kh√°c.
- Linh ho·∫°t: h·ªó tr·ª£ nhi·ªÅu chi·∫øn l∆∞·ª£c build.
- D·ªÖ d√πng: c√≥ CLI shp, t√≠ch h·ª£p trong Web Console.

C√°c CR quan tr·ªçng:
- Build: ƒë·ªãnh nghƒ©a c√°i g√¨ c·∫ßn build (ngu·ªìn code, chi·∫øn l∆∞·ª£c, output image).
- BuildStrategy / ClusterBuildStrategy: m√¥ t·∫£ c√°ch build (v√≠ d·ª• S2I, Buildah, Buildpacks).
- BuildRun: khi ch·∫°y build, s·∫Ω t·∫°o ra m·ªôt Pod th·ª±c hi·ªán build.

3. V√≠ d·ª• v·ªõi Buildah

Build resource (khai b√°o build):
```bash
apiVersion: shipwright.io/v1beta1
kind: Build
metadata:
  name: buildah-golang-build
spec:
  source:                 # code ngu·ªìn
    type: Git
    git:
      url: https://github.com/shipwright-io/sample-go
    contextDir: docker-build
  strategy:               # chi·∫øn l∆∞·ª£c build
    name: buildah
    kind: ClusterBuildStrategy
  paramValues:            # tham s·ªë build
  - name: dockerfile
    value: Dockerfile
  output:                 # image output
    image: image-registry.openshift-image-registry.svc:5000/buildah-example/sample-go-app
```

üëâ Gi·∫£i th√≠ch:
- source: l·∫•y code t·ª´ GitHub.
- strategy: d√πng Buildah.
- paramValues: ch·ªâ ra Dockerfile.
- output: image s·∫Ω ƒë∆∞·ª£c push v√†o internal registry.

BuildRun resource (ch·∫°y build):
```bash
apiVersion: shipwright.io/v1beta1
kind: BuildRun
metadata:
  name: buildah-golang-buildrun
spec:
  build:
    name: buildah-golang-build
```

üëâ BuildRun ch·ªâ c·∫ßn tham chi·∫øu ƒë·∫øn Build ƒë√£ ƒë·ªãnh nghƒ©a ‚Üí Shipwright t·∫°o Pod ƒë·ªÉ build.

4. CLI shp (thay cho YAML d√†i d√≤ng)

T·∫°o Build:
```bash
shp build create buildah-golang-build \
  --source-url="https://github.com/redhat-openshift-builds/samples" \
  --source-context-dir="buildah-build" \
  --strategy-name="buildah" \
  --dockerfile="Dockerfile" \
  --output-image="image-registry.openshift-image-registry.svc:5000/buildah-example/go-app"
```

Ch·∫°y build:
```
shp build run buildah-golang-build --follow
```
‚úÖ T√≥m t·∫Øt 

- BuildConfig: ki·ªÉu c≈©, thu·∫ßn OpenShift.
- Shipwright (Builds for OpenShift): ki·ªÉu m·ªõi, Kubernetes-native, linh ho·∫°t h∆°n.
- Build = ƒë·ªãnh nghƒ©a build, BuildStrategy = c√°ch build, BuildRun = ch·∫°y build.
- V√≠ d·ª•: l·∫•y code t·ª´ GitHub ‚Üí d√πng Buildah v·ªõi Dockerfile ‚Üí output th√†nh image trong registry.

**Builds using BuildConfig**

![alt text](pic/22.png)

**The S2I Build Workflow**

![alt text](pic/23.png)

‚úÖ T√≥m t·∫Øt

- Builder image: image ƒë·∫∑c bi·ªát ƒë·ªÉ build app t·ª´ source ‚Üí output image.
- S2I scripts: assemble (c√°ch build), run (c√°ch ch·∫°y).
- C√≥ 2 c√°ch d√πng:
  - T·∫°o builder image ch·ª©a s·∫µn script.
  - Override b·∫±ng .s2i/bin/ trong repo ‚Üí nhanh h∆°n, kh√¥ng c·∫ßn rebuild builder image.

## 4.3 Managing Application Builds
Create a Build Configuration
There are two ways to create a build configuration using the oc CLI: `oc new-app` and `oc new-build`.




## 4.5 Triggering Builds

## 4.7 Customizing an Existing S2I Base Image

---
# Chapter 5.  Managing Red Hat OpenShift Deployments

1. Kh√°i ni·ªám chung
- Deployment trong OpenShift t·ª± ƒë·ªông h√≥a qu√° tr√¨nh c·∫≠p nh·∫≠t ·ª©ng d·ª•ng.
- M·ª•c ti√™u: gi·∫£m downtime, tƒÉng t√≠nh ·ªïn ƒë·ªãnh, h·ªó tr·ª£ ph√°t h√†nh li√™n t·ª•c.
- Y√™u c·∫ßu app c·∫ßn tu√¢n th·ªß best practices:
  - X·ª≠ l√Ω t√≠n hi·ªáu SIGTERM ƒë·ªÉ t·∫Øt graceful.
  - Health/Readiness probe ƒë·ªÉ router ch·ªâ g·ª≠i request ƒë·∫øn pod kh·ªèe.

2. Deployment Resource
- L√† Kubernetes-native (d√πng ReplicaSet).
- T√≠nh nƒÉng: rollout theo config, scale, pause/resume rollout.
- DeploymentConfig (ki·ªÉu c≈©) ƒë√£ b·ªã deprecated.

3. Chi·∫øn l∆∞·ª£c Deployment
- C√≥ 2 nh√≥m:  
a. D·ª±a tr√™n Deployment Resource
- Rolling (RollingUpdate)
  - M·∫∑c ƒë·ªãnh.
  - T·ª´ng b∆∞·ªõc thay pod c≈© b·∫±ng pod m·ªõi.
  - Kh√¥ng downtime.
  - D√πng khi app ch·∫°y song song ƒë∆∞·ª£c nhi·ªÅu version.
- Recreate
  - X√≥a to√†n b·ªô pod c≈© ‚Üí ch·∫°y pod m·ªõi.
  - C√≥ downtime.
  - D√πng khi app kh√¥ng h·ªó tr·ª£ ch·∫°y song song, ho·∫∑c s·ª≠ d·ª•ng PVC v·ªõi RWO / RWOP.

b. D·ª±a tr√™n Router

1. Blue-Green
  - 2 m√¥i tr∆∞·ªùng (Blue ‚Äì m·ªõi, Green ‚Äì c≈©) ch·∫°y song song.
  - Route tr·ªè v√†o version n√†o th√¨ user d√πng version ƒë√≥.
  - D·ªÖ rollback.

2. A/B
- Route chia traffic theo t·ª∑ l·ªá (v√≠ d·ª• 10% Blue, 90% Green).
- D√πng ƒë·ªÉ test d·∫ßn, tƒÉng traffic t·ª´ t·ª´ cho version m·ªõi.

## 5.3 Managing Application Deployments
`oc rollout`  
The oc rollout command provides the cancel, pause, undo, retry, and more options for your deployments.
1. Xem tr·∫°ng th√°i rollout
```
oc rollout status deployment example-deployment
```
- D√πng ƒë·ªÉ theo d√µi ti·∫øn tr√¨nh tri·ªÉn khai (xem pod c≈© ƒë√£ xo√° ch∆∞a, pod m·ªõi ƒë√£ ch·∫°y ch∆∞a).
- K·∫øt qu·∫£ v√≠ d·ª•:
  - Waiting for deployment ... rollout to finish... ‚Üí ƒëang tri·ªÉn khai.
  - successfully rolled out ‚Üí tri·ªÉn khai th√†nh c√¥ng.

2. Rollback (quay l·∫°i version tr∆∞·ªõc)
```
oc rollout undo deployment example-deployment
```
- N·∫øu version m·ªõi b·ªã l·ªói, quay l·∫°i version c≈© ngay.
- Gi·ªëng nh∆∞ Ctrl+Z cho Deployment üòÑ.

3. Pause (t·∫°m d·ª´ng rollout)
```
oc rollout pause deployment example-deployment
```
- D√πng khi b·∫°n mu·ªën t·∫°m d·ª´ng tri·ªÉn khai t·ª± ƒë·ªông (v√≠ d·ª•: b·∫°n ƒëang s·ª≠a nhi·ªÅu config).
- L√∫c n√†y thay ƒë·ªïi kh√¥ng √°p d·ª•ng ngay.

4. Resume (ti·∫øp t·ª•c rollout)
```
oc rollout resume deployment example-deployment
```
- Sau khi ch·ªânh s·ª≠a xong, d√πng l·ªánh n√†y ƒë·ªÉ cho rollout ti·∫øp t·ª•c.

`oc scale`  
The oc scale command scales the number of replicas for a given deployment:
```
[user@host ~]$ oc scale deployment example-deployment --replicas=3
deployment.apps/example-deployment scaled
```

**Create Secrets and Configuration Maps**
Similarly to secrets, you can create configuration maps by using the oc create command:
```
[user@host ~]$ oc create configmap example-cm \
--from-literal key1=value1 \
--from-literal key2=value2
configmap/example-cm created
```
The previous command creates the following YAML object:
```
kind: ConfigMap
metadata:
    name: example-cm
apiVersion: v1
data:
    key1: value1
    key2: value2
```
You can also create configuration maps from a file or a directory:
```
[user@host ~]$ oc create configmap example-cm \
--from-file=redis.conf
configmap/example-cm created
```
The preceding example creates a configuration map with the redis.conf key and the contents of the file as its value. Developers might also rename the key, such as:
```bash
[user@host ~]$ oc create configmap example-cm \
--from-file=primary=/etc/redis/redis.conf \
--from-file=replica=replica-redis.conf
configmap/example-cm created
```
The preceding example creates a configuration map with the following keys:
- The primary key with the contents of the local /etc/redis/redis.conf file.
- The replica key with the contents of the local ./replica-redis.conf file.

Finally, similarly to secrets, you can use the OpenShift web console to create configuration maps. In the developer perspective, click ConfigMaps, select your project, and click Create ConfigMap.

**Manage Secrets and Configuration Maps**  
*View resources*  
To view details of a resource, use the oc get command:
```
[user@host ~]$ oc get secret mysecret -o yaml
...output omitted...
```
The -o yaml parameter displays the resource in the YAML language  
*Edit resources*  
To edit a resource, use the oc edit command:
```
[user@host ~]$ oc edit configmap my-cm
...output omitted...
```
Alternatively, you can edit resources in the OpenShift web console.  

*Patch resources*  
Patching a resource refers to updating the resource by applying a set of changes rather than interactively. This is useful, for example, in scripts. Use the oc patch command to patch a resource, for example:
```
[user@host ~]$ oc patch configmap/my-cm \
--patch '{"data":{"key1":"newvalue1"}}'
configmap/my-cm patched
```
The preceding command changes the .data.key1 key to the newvalue1 value.

The preceding commands work on any OpenShift resource. However, to edit secrets, you must use values in base64 encoding. You can encode any string by using the base64 command, for example:
```bash
[user@host ~]$ echo -n 'hunter3' | base64
aHVudGVyMw==

# decode
[user@host ~]$ echo -n 'aHVudGVyMw==' | base64 --decode
hunter3
```
*Inject Data to Pods*  
You can mount configuration maps and secrets as data volumes or expose the data as environment variables, inside an application container.

C√≥ 2 c√°ch ƒë·ªÉ ƒë∆∞a d·ªØ li·ªáu t·ª´ ConfigMap ho·∫∑c Secret v√†o trong Pod:
- Inject th√†nh bi·∫øn m√¥i tr∆∞·ªùng (env).
- Mount th√†nh file trong container.

1. Inject ConfigMap/Secret th√†nh Environment Variables

V√≠ d·ª• l·ªánh:
```
oc set env deployment my-deployment --from configmap/my-cm
```
- √ù nghƒ©a: L·∫•y t·∫•t c·∫£ key/value trong ConfigMap my-cm ‚Üí inject v√†o Deployment my-deployment ‚Üí th√†nh bi·∫øn m√¥i tr∆∞·ªùng trong container.
- K·∫øt qu·∫£ trong pod:
```
env:
  - name: KEY1
    valueFrom:
      configMapKeyRef:
        key: key1
        name: my-cm
```
- D√πng khi app ƒë·ªçc config t·ª´ ENV.

2. Mount ConfigMap/Secret th√†nh Volume (file trong container)

V√≠ d·ª• l·ªánh:
```
oc set volume deployment my-deployment --add \
-t secret -m /mnt/secret \
--name myvol --secret-name my-secret
```

√ù nghƒ©a: Mount Secret my-secret v√†o Deployment my-deployment, g·∫Øn t·∫°i /mnt/secret.

K·∫øt qu·∫£ trong pod:
```
volumeMounts:
- mountPath: /mnt/secret
  name: myvol
volumes:
- name: myvol
  secret:
    secretName: my-secret
```

D√πng khi app c·∫ßn file (v√≠ d·ª•: username.txt, password.txt, TLS cert‚Ä¶).

3. L∆∞u √Ω quan tr·ªçng
- ConfigMap/Secret ch·ªâ d√πng trong c√πng namespace (kh√¥ng share gi·ªØa project).
- N·∫øu b·∫°n c·∫≠p nh·∫≠t ConfigMap/Secret, pod ƒëang ch·∫°y kh√¥ng t·ª± ƒë·ªông nh·∫≠n gi√° tr·ªã m·ªõi.
  - B·∫°n ph·∫£i x√≥a pod ho·∫∑c rollout l·∫°i deployment ƒë·ªÉ pod m·ªõi nh·∫≠n gi√° tr·ªã update. Update config th√¨ c·∫ßn t√°i t·∫°o pod ƒë·ªÉ √°p d·ª•ng gi√° tr·ªã m·ªõi.

1. Service Account (SA) l√† g√¨?
- L√† identity (danh t√≠nh) cho ·ª©ng d·ª•ng trong OpenShift.
- C√≥ th·ªÉ g·∫Øn quy·ªÅn RBAC, secrets, SCC (Security Context Constraints)‚Ä¶ v√†o SA.
- M·∫∑c ƒë·ªãnh, m·ªçi pod d√πng default service account trong namespace.
- M·ªói SA c√≥ m·ªôt JWT token ƒë∆∞·ª£c mount v√†o pod t·∫°i:
```
/var/run/secrets/kubernetes.io/serviceaccount
```

‚Üí Pod c√≥ th·ªÉ d√πng token n√†y ƒë·ªÉ g·ªçi OpenShift API.

2. T·∫°o v√† g√°n Service Account
- T·∫°o SA:
```
oc create serviceaccount my-sa
```
- G√°n SA cho deployment:
```
oc set serviceaccount deployment nginx-deployment my-sa
```
K·∫øt qu·∫£: deployment nginx-deployment s·∫Ω ch·∫°y pod v·ªõi SA my-sa.

üëâ D√πng khi app c·∫ßn quy·ªÅn ƒë·∫∑c bi·ªát (v√≠ d·ª•: CI/CD pipeline ho·∫∑c operator g·ªçi API OpenShift).

3. Security Context & SCC
a. Security Context
- X√°c ƒë·ªãnh quy·ªÅn, UID, GID, capabilities m√† container ƒë∆∞·ª£c ph√©p.
- V√≠ d·ª•: ch·∫°y non-root, c·∫•m privilege escalation.

b. SCC (Security Context Constraints)
- L√† policy b·∫£o m·∫≠t ri√™ng c·ªßa OpenShift.
- T·ª± ƒë·ªông √°p d·ª•ng security context cho pod.
- M·∫∑c ƒë·ªãnh, pod th∆∞·ªùng d√πng SCC restricted-v2.

üëâ Admin c√≥ th·ªÉ g·∫Øn SCC v√†o SA ‚Üí Pod n√†o ch·∫°y v·ªõi SA ƒë√≥ s·∫Ω ƒë∆∞·ª£c √°p d·ª•ng SCC t∆∞∆°ng ·ª©ng.

4. V√≠ d·ª• Deployment v·ªõi Security Context
```
securityContext:
  runAsNonRoot: true
  allowPrivilegeEscalation: false
  seccompProfile:
    type: RuntimeDefault
  capabilities:
    drop:
    - ALL
```
- runAsNonRoot: true ‚Üí c·∫•m ch·∫°y user root.
- allowPrivilegeEscalation: false ‚Üí kh√¥ng cho container tƒÉng quy·ªÅn.
- capabilities.drop: ALL ‚Üí b·ªè h·∫øt Linux capabilities.
- seccompProfile: RuntimeDefault ‚Üí d√πng c·∫•u h√¨nh b·∫£o m·∫≠t m·∫∑c ƒë·ªãnh c·ªßa runtime.

5. L∆∞u √Ω quan tr·ªçng
- SA + SCC quy·∫øt ƒë·ªãnh pod ƒë∆∞·ª£c ph√©p l√†m g√¨.
- N·∫øu kh√¥ng ch·ªâ ƒë·ªãnh ‚Üí OpenShift s·∫Ω √°p d·ª•ng m·∫∑c ƒë·ªãnh (restricted-v2).
- ƒê·ªÉ ch·∫°y pod ƒë·∫∑c bi·ªát (v√≠ d·ª• c·∫ßn quy·ªÅn hostPath, privileged) ‚Üí c·∫ßn g√°n SCC ph√π h·ª£p cho SA.

## 5.5 Deploying Stateful Applications

**Persistent Volumes and Persistent Volume Claims**  
*Persistent Volumes*  
![alt text](pic/24.png)

*Persistent Volume Claims*  
Persistent volume claims (PVCs) represent a request for a persistent volume. These requests can include requirements for the PV, such as the following attributes:
- Amount of storage
- Label selector
- Volume mode
- Access mode
- Storage class

![alt text](pic/25.png)

**Static and Dynamic Provisioning**

1. Static Provisioning
- Admin t·∫°o PV th·ªß c√¥ng tr∆∞·ªõc ‚Üí Developer ch·ªâ t·∫°o PVC ƒë·ªÉ claim v√†o PV ƒë√£ c√≥.
- Nh∆∞·ª£c ƒëi·ªÉm:
  - T·ªën c√¥ng qu·∫£n l√Ω (Admin ph·∫£i ƒëo√°n tr∆∞·ªõc dung l∆∞·ª£ng).
  - D·ªÖ l√£ng ph√≠ n·∫øu PV kh√¥ng ƒë∆∞·ª£c d√πng.
- D√πng khi storage backend kh√¥ng h·ªó tr·ª£ dynamic ho·∫∑c m√¥i tr∆∞·ªùng lab/test.

2. Dynamic Provisioning

- Developer ch·ªâ c·∫ßn t·∫°o PVC.
- Cluster s·∫Ω t·ª± ƒë·ªông t·∫°o PV m·ªõi ph√π h·ª£p nh·ªù StorageClass (SC).
- ƒêi·ªÅu ki·ªán:
  - Admin ph·∫£i c·∫•u h√¨nh StorageClass v·ªõi m·ªôt Provisioner plugin (v√≠ d·ª•: nfs-subdir-external-provisioner, Ceph, EBS, v.v.).
  - N·∫øu default SC ch∆∞a ƒë·ªãnh nghƒ©a, PVC ph·∫£i ch·ªâ r√µ storageClassName.

3. StorageClass (SC)
- L√† ‚Äúprofile‚Äù c·ªßa storage: ƒë·ªãnh nghƒ©a lo·∫°i storage, t·ªëc ƒë·ªô, reclaim policy,‚Ä¶
- V√≠ d·ª• SC:
```
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: nfs-storage
  annotations:
    storageclass.kubernetes.io/is-default-class: "true"
provisioner: k8s-sigs.io/nfs-subdir-external-provisioner
reclaimPolicy: Delete
volumeBindingMode: Immediate
```
- provisioner: plugin lo vi·ªác c·∫•p ph√°t storage.
- reclaimPolicy: Delete (xo√° PV khi xo√° PVC) ho·∫∑c Retain (gi·ªØ l·∫°i PV).
- is-default-class: "true" ‚Üí PVC n√†o kh√¥ng ghi storageClassName th√¨ m·∫∑c ƒë·ªãnh d√πng SC n√†y.

4. PVC v·ªõi Dynamic Provisioning

V√≠ d·ª•:
```
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: dynamic-volume-claim
spec:
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 1Gi
  storageClassName: nfs-storage
```
- Khi t·∫°o PVC n√†y ‚Üí SC nfs-storage s·∫Ω t·∫°o ra m·ªôt PV m·ªõi t·ª± ƒë·ªông ‚Üí PVC ƒë∆∞·ª£c bind.
- Admin kh√¥ng c·∫ßn t·∫°o PV th·ªß c√¥ng.

So s√°nh
| ƒê·∫∑c ƒëi·ªÉm    | Static Provisioning   | Dynamic Provisioning            |
| ----------- | --------------------- | ------------------------------- |
| Ai t·∫°o PV   | Admin                 | T·ª± ƒë·ªông (qua StorageClass)      |
| Ai t·∫°o PVC  | Developer             | Developer                       |
| Linh ho·∫°t   | K√©m                   | Cao                             |
| Th∆∞·ªùng d√πng | Test, storage c·ªë ƒë·ªãnh | Production, CI/CD, Cloud-native |

**Mounting Claims Within Pods**

Mount PVC v√†o Pod

![alt text](pic/26.png)

Mount PVC v√†o Deployment

![alt text](pic/27.png)

K·∫øt qu·∫£: Deployment YAML ƒë∆∞·ª£c c·∫≠p nh·∫≠t th√™m:
```bash
volumeMounts:
- mountPath: /tmp/data
  name: nfs-volume-storage

volumes:
- name: nfs-volume-storage
  persistentVolumeClaim:
    claimName: my-data-claim
```
3. L∆∞u √Ω khi d√πng PVC trong Deployment
- T·∫•t c·∫£ pod replica trong Deployment ƒë·ªÅu mount c√πng PVC ‚Üí c√≥ th·ªÉ g√¢y conflict n·∫øu ·ª©ng d·ª•ng kh√¥ng h·ªó tr·ª£ shared storage.
- V·ªõi database (MySQL, PostgreSQL, MongoDB, ‚Ä¶):
  - M·ªói instance th∆∞·ªùng c·∫ßn storage ri√™ng.
  - Replication/sharding do ch√≠nh database qu·∫£n l√Ω, kh√¥ng d√πng chung PVC.
- V·ªõi stateless app (web server, API, ‚Ä¶): th∆∞·ªùng kh√¥ng c·∫ßn PVC ‚Üí d·ªÖ scale h∆°n.

4. Ephemeral Storage (t·∫°m th·ªùi)

- D√πng volume ƒë·ªÉ inject file c√≥ s·∫µn m√† kh√¥ng c·∫ßn rebuild image.
- V√≠ d·ª•:
  - Script init database.
  - Config server.
  - TLS cert/token/key.
- ConfigMap & Secret c≈©ng c√≥ th·ªÉ mount d∆∞·ªõi d·∫°ng volume ‚Üí r·∫•t ph·ªï bi·∫øn.
- D·ªØ li·ªáu ephemeral s·∫Ω m·∫•t khi pod b·ªã xo√° (kh√¥ng persistent).

**Stateful Sets**

As their name suggests, stateful sets are intended for stateful applications. Unlike deployments, pods within stateful sets are guaranteed to have a predictable identifier (ID) for each pod. For example, three replicas for the redis stateful set might use the redis-0, redis-1, and redis-2 pod names. This is useful for routing requests to each pod or discovering new pods that must join to an existing cluster. In addition, each replica pod created by a stateful set can have a dedicated PVC.

![alt text](pic/28.png)
1. B√¨nh th∆∞·ªùng (Service c√≥ ClusterIP)
- Khi b·∫°n t·∫°o Service b√¨nh th∆∞·ªùng (ClusterIP ‚â† None), Kubernetes/OpenShift s·∫Ω load balance traffic ƒë·∫øn b·∫•t k·ª≥ Pod n√†o match label selector.
- C√°c Pod trong Deployment th∆∞·ªùng gi·ªëng h·ªát nhau, n√™n vi·ªác ‚Äúg·ª≠i v√†o Pod n√†o c≈©ng ƒë∆∞·ª£c‚Äù ‚Üí kh√¥ng v·∫•n ƒë·ªÅ.

2. Headless Service (ClusterIP = None)
```
apiVersion: v1
kind: Service
metadata:
  name: my-stateful-app
spec:
  clusterIP: None
  selector:
    app: my-stateful-app
```
- clusterIP: None ‚Üí Service kh√¥ng c√≥ IP ri√™ng, kh√¥ng l√†m load balancing.
- Thay v√†o ƒë√≥, m·ªói Pod trong StatefulSet c√≥ DNS ri√™ng.
- V√≠ d·ª• StatefulSet t·∫°o 3 replica:
```
my-stateful-app-0.my-stateful-app

my-stateful-app-1.my-stateful-app

my-stateful-app-2.my-stateful-app
```
M·ªói Pod c√≥ ƒë·ªãa ch·ªâ ri√™ng, c√≥ th·ªÉ g·ªçi tr·ª±c ti·∫øp.

## Monitoring Application Health

*Specifying Application Resource Requirements*

![alt text](image.png)

**Types of Probes**  
1. Startup Probe
- Ki·ªÉm tra: ·ª©ng d·ª•ng trong container ƒë√£ kh·ªüi ƒë·ªông th√†nh c√¥ng ch∆∞a.
- Ch·ªâ ch·∫°y l√∫c startup, 1 l·∫ßn cho ƒë·∫øn khi th√†nh c√¥ng ‚Üí sau ƒë√≥ m·ªõi k√≠ch ho·∫°t c√°c probe kh√°c.
- N·∫øu fail ‚Üí container b·ªã kill v√† restart (theo restartPolicy).
- D√πng khi: ·ª©ng d·ª•ng kh·ªüi ƒë·ªông l√¢u (VD: Java app, DB server‚Ä¶).

üëâ C·∫•u h√¨nh: spec.containers.startupProbe

2. Readiness Probe

- Ki·ªÉm tra: container c√≥ s·∫µn s√†ng nh·∫≠n traffic ch∆∞a.
- N·∫øu fail ‚Üí Pod b·ªã lo·∫°i kh·ªèi Service endpoint, kh√¥ng nh·∫≠n request cho ƒë·∫øn khi pass.
- D√πng khi: ·ª©ng d·ª•ng c·∫ßn l√†m vi·ªác chu·∫©n b·ªã (m·ªü k·∫øt n·ªëi DB, load config/cache).
- Kh√¥ng kill container, ch·ªâ ‚Äúƒë·ª©ng ngo√†i load balancer‚Äù.

üëâ C·∫•u h√¨nh: spec.containers.readinessProbe

3. Liveness Probe
- Ki·ªÉm tra: ·ª©ng d·ª•ng trong container c√≤n ch·∫°y kh·ªèe m·∫°nh kh√¥ng.
- N·∫øu fail ‚Üí OpenShift/K8s s·∫Ω restart container.
- D√πng khi: ·ª©ng d·ª•ng c√≥ th·ªÉ b·ªã treo (deadlock, memory leak).

üëâ C·∫•u h√¨nh: spec.containers.livenessProbe

![alt text](pic/30.png)

**Methods of Checking Application Health**  
Startup, readiness, and liveness probes can verify that an application is in a healthy state in the following three ways:
- HTTP checks
- Container execution checks
- TCP socket checks

HTTP Checks
![alt text](pic/31.png)

Container Execution Checks

![alt text](pic/32.png)

TCP Socket Checks

![alt text](pic/33.png)

**Managing Probes**  
Developers can create and manage probes with either the oc CLI client or the OpenShift web console.

![alt text](pic/34.png)

![alt text](pic/35.png)

**Probes Via the CLI**  
The oc set probe command creates probes on existing workloads.
```
[user@host ~]$ oc set probe deployment/myapp \
--readiness \
--get-url=http://:8080/readyz \
--period-seconds=20
```
```
[user@host ~]$ oc set probe deployment/myapp \
--liveness \
--open-tcp=3306 \
--period-seconds=20 \
--timeout-seconds=1
```
```
[user@host ~]$ oc set probe deployment/myapp \
--liveness \
--get-url=http://:8080/livez \
--initial-delay-seconds=30 \
--success-threshold=1 \
--failure-threshold=3
```
Use the oc set probe --help command to view the available options.

**Automatically Scaling Applications**   
Horizontal Scaling

![alt text](pic/36.png)

Vertical Scaling


---
# Chapter 6.  Deploying Multi-container Applications

üéØ OpenShift Template d√πng ƒë·ªÉ l√†m g√¨?
- ƒê√≥ng g√≥i nhi·ªÅu resource l·∫°i th√†nh 1 g√≥i ‚Üí d·ªÖ tri·ªÉn khai.
V√≠ d·ª•: Deployment + Service + Route + PVC ‚Üí gom chung v√†o 1 file Template.
- D√πng tham s·ªë (parameters) ‚Üí linh ho·∫°t khi deploy.
V√≠ d·ª•: thay ƒë·ªïi t√™n app, image, s·ªë replicas, ho·∫∑c k√≠ch th∆∞·ªõc storage m√† kh√¥ng c·∫ßn s·ª≠a tay nhi·ªÅu ch·ªó trong YAML.
- T·ª± ƒë·ªông h√≥a v√† t√°i s·ª≠ d·ª•ng ‚Üí QA, Dev, Ops ch·ªâ c·∫ßn 1 l·ªánh oc new-app -f template.yaml -p PARAM=VALUE l√† deploy ra nguy√™n c·ª•m app.
- D√πng cho m√¥i tr∆∞·ªùng multi-tier (web + app + DB) ‚Üí deploy nhanh c·∫£ h·ªá th·ªëng thay v√¨ t·ª´ng b∆∞·ªõc.
- ISV (nh√† cung c·∫•p ph·∫ßn m·ªÅm) hay d√πng template ƒë·ªÉ ship s·∫£n ph·∫©m ‚Üí kh√°ch h√†ng ch·ªâ c·∫ßn ch·∫°y template l√† c√≥ ƒë·ªß c√°c th√†nh ph·∫ßn.

![alt text](pic/37.png)

**Creating an Application from a Template**  
You can deploy an application directly from a template definition file. The oc new-app and oc process commands can use a template as an input and process this file to create resources.

The oc new-app command can process a template file to create resources in OpenShift, as follows:
```
[user@host ~]$ oc new-app --file mytemplate.yaml -p PARAM1=value1 \
-p PARAM2=value2
```
You can also use a template stored in the cluster. This example processes a template called mysql-persistent:
```
[user@host ~]$ oc new-app --template mysql-persistent \
-p MYSQL_USER=student -p MYSQL_PASSWORD=mypass
```
The oc process command processes a template and produces a resource list. You can save the resource list to a local file as follows:
```
[user@host ~]$ oc process -f mytemplate.yaml -p PARAM1=value1 \
-p PARAM2=value2 > myresourcelist.json
```
Note
You can modify the output format, which is JSON by default, by using the --output (-o) option.

## 6.3 Install Applications by Using Helm Charts

![alt text](pic/38.png)

**Deploy Charts with Web Console**

**Manage Charts with Helm CLI**
```
[user@host ~]$ helm repo add openshift-helm-charts \
https://charts.openshift.io/
"openshift-helm-charts" has been added to your repositories
```
When you add a private registry, you can define the credentials when adding the repository, for example by using the `--username` and `--password` flags.

You can search the contents of the repository:
```
[user@host ~]$ helm search repo openshift-helm-charts
NAME                                              	CHART VERSION	APP VERSION 	DESCRIPTION
openshift-helm-charts/a10tkc                      	0.2.0        	1.16.0        A Helm chart for A10 Thunder Kubernetes Connector
openshift-helm-charts/akeyless-api-gateway        	1.41.2       	4.4.1         A Helm chart for Kubernetes that deploys akeyle...
openshift-helm-charts/alaz                        	0.5.0        	v0.5.2        Alaz is an open-source Ddosify eBPF agent that ...
...output omitted...
```

Use the `helm pull` command to download a helm chart.
```
[user@host ~]$ helm pull openshift-helm-charts/redhat-quarkus \
--untar --destination redhat-quarkus
...no output expected...
```

To upload your charts, use the helm package command to create a chart tar file.
```
[user@host ~]$ helm package my-chart-directory
Successfully packaged chart and saved it to: /home/user/example-chart-0.1.0.tgz
```
Then, use the helm push command to upload the packaged helm chart to your repository.
```
[user@host ~]$ helm push example-chart-0.1.0.tgz example.repository.org
...output omitted...
```
Finally, use the helm install command to install a remote or local chart.
```
[user@host ~]$ helm install my-quarkus-application \
openshift-helm-charts/redhat-quarkus \
--set replicaCount=3,image.tag=latest
...output omitted...
Your Quarkus app is building! To view the build logs, run:

oc logs bc/my-quarkus-application --follow
...output omitted...
```

So sanh
| L·ªánh | M·ª•c ƒë√≠ch |
| --- | --- |
| `helm pull`    | Ch·ªâ **t·∫£i chart** v·ªÅ local, ch∆∞a c√†i g√¨ l√™n cluster.   |
| `helm install` | **Tri·ªÉn khai chart** th√†nh ·ª©ng d·ª•ng th·∫≠t tr√™n cluster. |

You can view Helm releases, or applications that you installed by using Helm charts:
```
[user@host ~]$ helm ls
NAME                  	NAMESPACE  REVISION	UPDATED   STATUS  	CHART
my-quarkus-application	test	     1       	2022-...	deployed	quarkus-0.0.3
vertx-app             	test	     1       	2023-...	deployed	ver
```
When the chart developer releases an updated chart, you can update your release:
```
[user@host ~]$ helm upgrade my-quarkus-application \
openshift-helm-charts/redhat-quarkus
Release "my-quarkus-application" has been upgraded. Happy Helming!
NAME: my-quarkus-application
LAST DEPLOYED: Thu Aug 17 08:37:07 2023
NAMESPACE: multicontainer-helm
STATUS: deployed
REVISION: 2
TEST SUITE: None
```
Use helm history to list the release application history:
```
[user@host ~]$ helm history my-quarkus-application
...output omitted...
REVISION	UPDATED                 	STATUS    	CHART        	... 	DESCRIPTION
1       	Thu Jun 06 08:36:13 2022	superseded	quarkus-0.0.2	 	    Install complete
2       	Thu Aug 17 08:37:07 2023	deployed  	quarkus-0.0.3	 	    Upgrade complete
```
If the new chart version does not work as you expect, you can roll back to the previous version. The following example rolls back the my-quarkus-application release to revision 1.
```
[user@host ~]$ helm rollback my-quarkus-application 1
Rollback was a success! Happy Helming!
```
Finally, you can uninstall the release:
```
[user@host ~]$ helm uninstall my-quarkus-application
release "my-quarkus-application" uninstalled
```

**Creating Helm Charts**
To generate a Helm chart directory structure, use the helm create command:
```
[user@host ~]$ helm create my-helm-chart
Creating my-helm-chart
```
The previous command creates the following directory structure:
```
[user@host ~]$ tree my-helm-chart
my-helm-chart/
‚îú‚îÄ‚îÄ Chart.yaml
‚îú‚îÄ‚îÄ charts
‚îú‚îÄ‚îÄ templates
‚îÇ   ‚îú‚îÄ‚îÄ NOTES.txt
‚îÇ   ‚îú‚îÄ‚îÄ _helpers.tpl
‚îÇ   ‚îú‚îÄ‚îÄ deployment.yaml
‚îÇ   ‚îú‚îÄ‚îÄ hpa.yaml
‚îÇ   ‚îú‚îÄ‚îÄ ingress.yaml
‚îÇ   ‚îú‚îÄ‚îÄ service.yaml
‚îÇ   ‚îú‚îÄ‚îÄ serviceaccount.yaml
‚îÇ   ‚îî‚îÄ‚îÄ tests
‚îÇ       ‚îî‚îÄ‚îÄ test-connection.yaml
‚îî‚îÄ‚îÄ values.yaml
```
Charts contain the following important components:

`Chart.yaml`   
This is the main chart file that contains the chart metadata. For example, it defines the chart name, its description, and version.

`values.yaml`  
The values.yaml file contains variables that you can use to template your YAML files, for example:
```
replicaCount: 1

image:
  repository: nginx
  pullPolicy: IfNotPresent
```
`templates`  
The templates directory holds the YAML files that you want to template and deploy. By default, Helm deploys all YAML files that are present in this directory.

`templates/NOTES.txt`  
This file configures the text that Helm prints after you install the chart. Typically, this file contains information about the deployed application, such as the application URL, or information about how developers can interact with the application.

**Verify Templates**  
When you create templates, it is useful to verify that the templates are syntactically correct, which means that Helm can render the template. Use the helm template command to render all templates in the chart, for example:
```
[user@host ~]$ helm template my-helm-chart
---
# Source: my-helm-chart/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
...output omitted...
```
To render a specific template, use the --show-only parameter. The short -s parameter is an alternative to the --show-only parameter.
```
[user@host ~]$ helm template -s templates/serviceaccount.yaml my-helm-chart
---
# Source: my-helm-chart/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
...output omitted...
```
Similarly to installing a chart, you can template a remote chart as well, for example:
```
[user@host ~]$ helm template openshift-helm-charts/redhat-quarkus
---
# Source: quarkus/templates/service.yaml
apiVersion: v1
kind: Service
...output omitted...
```
This is useful to inspect the chart YAML files before deploying the chart without downloading the chart locally.

**Template YAML Files With Helm**

üîπ C√°ch ho·∫°t ƒë·ªông
- B·∫°n c√≥ `values.yaml`: ch·ª©a c√°c bi·∫øn c·∫•u h√¨nh.
- B·∫°n c√≥ file trong templates/: ch·ª©a YAML + c√∫ ph√°p template ({{ }}) ƒë·ªÉ l·∫•y gi√° tr·ªã t·ª´ values.yaml.
- Khi ch·∫°y helm install, Helm s·∫Ω thay th·∫ø bi·∫øn b·∫±ng gi√° tr·ªã th·∫≠t t·ª´ values.yaml r·ªìi apply.

V√≠ d·ª• 1: D√πng bi·∫øn c∆° b·∫£n

üìÑ values.yaml
```
replicaCount: 3
image:
  repository: quay.io/example/deployment
  tag: "1.0"
```

üìÑ templates/deployment.yaml
```
apiVersion: apps/v1
kind: Deployment
metadata:
  name: example-deployment
spec:
  replicas: {{ .Values.replicaCount }}
  template:
    spec:
      containers:
      - name: example-deployment
        image: {{ .Values.image.repository }}:{{ .Values.image.tag }}
```

üëâ Khi render, Helm t·∫°o ra Deployment v·ªõi 3 replicas, image ƒë√∫ng nh∆∞ trong values.yaml.

V√≠ d·ª• 2: D√πng with
```
{{ with .Values.image }}
containers:
- name: example-deployment
  image: {{ .repository }}:{{ .tag }}
{{ end }}
```

·ªû ƒë√¢y, with .Values.image nghƒ©a l√† b·∫°n zoom v√†o scope image. B√™n trong, .repository = image.repository.

V√≠ d·ª• 3: D√πng h√†m built-in
```
imagePullPolicy: {{ .Values.image.pullPolicy | default "Always" | quote }}
```
- default "Always": n·∫øu pullPolicy kh√¥ng c√≥, th√¨ d√πng gi√° tr·ªã m·∫∑c ƒë·ªãnh "Always".
- quote: b·ªçc chu·ªói trong d·∫•u ".

V√≠ d·ª• 4: D√πng ƒëi·ªÅu ki·ªán if
```
{{ if eq .Values.createSharedSecret "true" }}
env:
  - name: DATABASE_USER
    valueFrom:
      secretKeyRef:
        name: postgresql
        key: database-user
{{ end }}
```

üëâ Nghƒ©a l√†: ch·ªâ th√™m env v√†o container n·∫øu trong values.yaml c√≥:
```
createSharedSecret: "true"
```

N·∫øu false ho·∫∑c kh√¥ng c√≥ ‚Üí ƒëo·∫°n YAML ƒë√≥ s·∫Ω b·ªã b·ªè qua.

‚úÖ T√≥m l·∫°i:
- Helm template cho ph√©p bi·∫øn h√≥a YAML ƒë·ªông: th√™m/b·ªõt, thay gi√° tr·ªã, ƒëi·ªÅu ki·ªán‚Ä¶
- Nh·ªù ƒë√≥ 1 chart c√≥ th·ªÉ deploy cho nhi·ªÅu m√¥i tr∆∞·ªùng (dev, staging, prod) ch·ªâ b·∫±ng c√°ch ƒë·ªïi values.yaml, kh√¥ng c·∫ßn vi·∫øt l·∫°i YAML t·ª´ ƒë·∫ßu.

## 6.5 The Kustomize CLI

The following directory structure shows an example of a Kustomize directory layout:
```
myapp
‚îú‚îÄ‚îÄ base
‚îî‚îÄ‚îÄ overlays
  ‚îú‚îÄ‚îÄ production
  ‚îî‚îÄ‚îÄ staging
```

Resources Files
The resources section of the kustomization.yaml file is a list of files that create all the resources for a specific environment, for example:

resources:
- deployment.yaml
- secrets.yaml
- service.yaml
The layout for a base definition with the preceding resources uses the following directory structure:
```
myapp
‚îî‚îÄ‚îÄ base
  ‚îú‚îÄ‚îÄ deployment.yaml
  ‚îú‚îÄ‚îÄ kustomization.yaml
  ‚îú‚îÄ‚îÄ secrets.yaml
  ‚îî‚îÄ‚îÄ service.yaml
```
Separating Kubernetes object definitions into smaller files simplifies maintenance of the base set.

Overlay Configuration
The kustomization.yaml file in an overlay directory must point to one or multiple base configuration sets that form the starting point for the overlay, for example:
```
resources:
- ../../base
```
This kustomization.yaml file is placed in an overlay, for example:
```
myapp/
‚îú‚îÄ‚îÄ base
‚îÇ   ‚îú‚îÄ‚îÄ deployment.yaml
‚îÇ   ‚îú‚îÄ‚îÄ kustomization.yaml
‚îÇ   ‚îú‚îÄ‚îÄ secrets.yaml
‚îÇ   ‚îî‚îÄ‚îÄ service.yaml
‚îî‚îÄ‚îÄ overlays
    ‚îî‚îÄ‚îÄ staging
        ‚îî‚îÄ‚îÄ kustomization.yaml
```
This overlay starts with the base configuration and then applies any patch defined in it.  
**Multi-container Deployments Comparison**  
![alt text](pic/39.png)

---
# Chapter 7.  Continuous Deployment by Using Red Hat OpenShift Pipelines

![alt text](pic/40.png)


Pipelines Workflow

![alt text](pic/41.png)

To start a pipeline or a task, you can use the tkn CLI or the Pipelines section of the Web console, available both in the developer and administrator perspectives.

![alt text](pic/43.png)

**Differences with Jenkins**
![alt text](pic/42.png)


## Creating CI/CD Workflows by Using Red Hat OpenShift Pipelines
**Defining Custom Tasks**

![alt text](pic/44.png)

Default Tasks

```
[user@host ~]$ oc get task/buildah \
-n openshift-pipelines \
-o yaml
apiVersion: tekton.dev/v1
kind: Task
metadata:
  name: buildah
```

Alternatively, you can use the Tekton CLI (tkn). The following example command shows metadata about the same buildah task.
```
[user@host ~]$ tkn -n openshift-pipelines t describe buildah
Name:          buildah
Namespace:     openshift-pipelines
Description:   Buildah task builds source into a container image and
then pushes it to a container registry.
...output omitted...

Params
```

![alt text](pic/45.png)

![alt text](pic/46.png)













