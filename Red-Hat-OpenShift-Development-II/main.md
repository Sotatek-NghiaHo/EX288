# Chapter 1.  Red Hat OpenShift Container Platform for Developers


![alt text](pic/1.png)
Control Plane

Kubernetes services
- etcd: the distributed key-value store, which Kubernetes uses to store configuration and state information about the containers and other resources inside the cluster.
- kube-apiserver: validates and configures cluster objects and provides the access point to the shared state of the cluster.
- kube-controller-manager: monitors etcd for changes and uses the Kubernetes API to apply changes to the cluster.
- kube-scheduler: selects the nodes where a workload must run.

## Red Hat OpenShift Concepts and Terminology

### Kubernetes Concepts

**Pods**  
A Pod is a collection of containers that share the same storage and network. Pods share the context by using Linux namespaces, cgroups, and other isolation technologies.

Each container in a pod usually contains applications that are more or less logically coupled.

**ReplicaSet**  
The ReplicaSet object indicates the number of pods that are available to attend a request. This object also ties all the pods replicas together so you can operate on them at the same time.

**Deployments**  
A Deployment contains the desired state of an application's pods and uses a ReplicaSet to achieve this desired state. Some changes in the application's state can be: creating pods, declaring a new state of pods, changing the number of pods, or rolling back to a previous Deployment revision.

**Service**  
A Kubernetes Service exposes a set of pods over a network. This abstraction allows internal or external clients of the application running on said pods to connect to them regardless of the actual state of the replicas or varying network IPs.

**Ingress**  
An Ingress exposes services inside the cluster to outside clients by using HTTP or HTTPS. A service ingress can also provide external URLs, load balancing, name-based virtual hosting, or SSL/TLS termination.

**Namespace**  
A Namespace can enable you to isolate resources, encapsulate objects under a unique name, and provide resource quotas.

**Custom Resource**  
Custom Resource (CR) allows extending the Kubernetes API. Custom resources represent entities other than the default ones in Kubernetes. Additionally, Custom resources interact with other cluster objects, regardless of whether those other objects are default or custom.

**Operator**  
An Operator is a custom Kubernetes controller that uses custom resources to deploy and manage applications. It takes high-level user configuration and acts to make the cluster match the desired state.

**Service Account**    
A Service Account is a special kind of account that does not correspond to an actual user, but it is used internally by cluster tools. It is useful for pods to connect to objects in the cluster, such as CI/CD pipelines, secrets, or external resources (outside of the namespace or the cluster).

**Storage Class**   
A Storage Class is a name that identifies a particular kind of storage defined by the cluster administrator. A storage class also defines its characteristics, such as backup policies, service level quality, or any other specification the administrator might choose.

**Persistent Volume**  
A Persistent Volume (PV), is a persistence storage unit offered by the cluster, independent of cluster nodes. This object holds information regarding the size, type, or ability to share storage.

**Persistent Volume Claim**  
Users claim the storage that a PV offers by using a Persistent Volume Claim (PVC). A PVC is a request to access a specific kind of storage of the required size. After acquiring the PVC, the storage is attached to the pods claiming it.

---
# Chapter 2.  Deploying Simple Applications

![alt text](pic/2.png)

![alt text](pic/3.png)

Deploying Applications by Using the Red Hat OpenShift Web Console
- Deploy an application by using a Git repository.
- Deploy an application by using an image from a container registry.

![alt text](pic/5.png)

![alt text](pic/6.png)


## Deploying Applications by Using the oc and odo CLIs



**Deploying Applications with the OpenShift CLI**   
For complex applications, you can use the oc apply command with the -f option. You provide a manifest file that defines the Kubernetes resources for your application.
```
[user@host ~]$ oc apply -f my-manifest.yaml
```
Deploying Applications with oc new-app

![alt text](pic/4.png)

To get a complete list of options and to see a list of examples, run the `oc new-app -h` command.

Example

![alt text](pic/8.png)

![alt text](pic/9.png)

You can delete resources that the oc new-app command creates by using a single oc delete command with the --selector option and the app label. The following command deletes the resources created by the previous oc new-app command:
```
[user@host ~]$ oc delete all --selector app=hello
```

Use the `-o` option to inspect resource definitions without creating resources.
```
[user@host ~]$ oc new-app \
-o yaml registry.example.com/mycontainerimage
apiVersion: v1
items:
- apiVersion: image.openshift.io/v1
  kind: ImageStream
  ...output omitted...
- apiVersion: build.openshift.io/v1
  kind: BuildConfig
  ...output omitted...
- apiVersion: apps/v1
  kind: Deployment
  ...output omitted...
- apiVersion: v1
  kind: Service
  ...output omitted...
kind: List
metadata: {}
```
**Exposing Applications Outside the Cluster.**  
Service resources are only accessible from within the cluster. To provide external access to your application, you can use the oc expose command.

The `oc expose` command creates a Route resource, a type of OpenShift-specific resource. This resource defines the port and protocol for access outside the cluster.

Syntax
```
oc expose [RESOURCE/NAME] [options]
```
Trong Ä‘Ã³:

- RESOURCE/NAME: loáº¡i resource vÃ  tÃªn (thÆ°á»ng lÃ  service/<service-name> hoáº·c deployment/<deploy-name>).
- [options]: cÃ¡c tÃ¹y chá»n Ä‘á»ƒ Ä‘á»‹nh nghÄ©a route hoáº·c service.


Má»™t sá»‘ option quan trá»ng

- `--name=<string>`
â†’ Äáº·t tÃªn cho resource má»›i Ä‘Æ°á»£c táº¡o (vÃ­ dá»¥: route).

- `--port=<port-name|number>`
â†’ Chá»‰ Ä‘á»‹nh cá»•ng tá»« Service mÃ  Route sáº½ expose (náº¿u Service cÃ³ nhiá»u port).

- `--target-port=<number>`
â†’ Chá»‰ Ä‘á»‹nh chÃ­nh xÃ¡c cá»•ng Ä‘Ã­ch trÃªn Pod container.

- `--hostname=<string>`
â†’ Äáº·t hostname (FQDN) cho Route. Náº¿u khÃ´ng cÃ³, OpenShift sáº½ generate.

- `--path=<string>`
â†’ Gáº¯n path cho Route (vÃ­ dá»¥ /api).

- `--type=<string>`
â†’ Loáº¡i service muá»‘n expose (ClusterIP, NodePort, LoadBalancer).

- `--generator=<string>`
â†’ Kiá»ƒu resource generator. Máº·c Ä‘á»‹nh khi expose Service lÃ  route/v1

CÃº phÃ¡p Ä‘áº§y Ä‘á»§ nháº¥t (hay dÃ¹ng):
```
oc expose service <service-name> \
  --name=<route-name> \
  --port=<service-port> \
  --hostname=<custom-host> \
  --path=<url-path>
```
ğŸ”¹ `--hostname=<custom-host>`

- DÃ¹ng khi báº¡n muá»‘n Route cÃ³ hostname cá»¥ thá»ƒ thay vÃ¬ hostname ngáº«u nhiÃªn OpenShift generate.
- TrÆ°á»ng há»£p dÃ¹ng:
  - Báº¡n muá»‘n public domain dá»… nhá»›, vÃ­ dá»¥:
```
oc expose service myservice --hostname=weather.apps.ocp.example.com
```
- Khi báº¡n Ä‘Ã£ cÃ³ DNS record trá» vá» OpenShift router (Ingress).
- Trong mÃ´i trÆ°á»ng production, hostname thÆ°á»ng Ä‘Æ°á»£c quy Ä‘á»‹nh trÆ°á»›c (VD: api.company.com, shop.company.com).

ğŸ‘‰ Náº¿u khÃ´ng set, OpenShift sáº½ tá»± táº¡o tÃªn dáº¡ng:
```
<route-name>-<project>.apps.<cluster-domain>
```
ğŸ”¹`--path=<url-path>`
- DÃ¹ng Ä‘á»ƒ thÃªm context path cho route.
- TrÆ°á»ng há»£p dÃ¹ng:
  - Khi báº¡n muá»‘n nhiá»u á»©ng dá»¥ng share chung 1 hostname, nhÆ°ng phÃ¢n biá»‡t báº±ng path.
VÃ­ dá»¥:
```
oc expose service backend --hostname=api.apps.ocp.example.com --path=/backend
oc expose service frontend --hostname=api.apps.ocp.example.com --path=/frontend
```
- Khi á»©ng dá»¥ng cá»§a báº¡n láº¯ng nghe trÃªn root /, nhÆ°ng báº¡n chá»‰ muá»‘n expose dÆ°á»›i má»™t nhÃ¡nh /app1 cháº³ng háº¡n.

ğŸ‘‰ Náº¿u khÃ´ng set, máº·c Ä‘á»‹nh path lÃ  `/` (root).

- `--hostname` â†’ dÃ¹ng khi báº¡n muá»‘n route cÃ³ domain cá»¥ thá»ƒ, nháº¥t lÃ  trong production (gáº¯n DNS chuáº©n).

- `--path` â†’ dÃ¹ng khi muá»‘n cháº¡y nhiá»u service/app dÆ°á»›i cÃ¹ng 1 domain nhÆ°ng phÃ¢n biá»‡t báº±ng Ä‘Æ°á»ng dáº«n.


OpenShift chá»n port nÃ o Ä‘á»ƒ expose khi dung `oc expose` command ? 
- oc expose service ... máº·c Ä‘á»‹nh láº¥y port tá»« Service.
- Náº¿u Service cÃ³ 1 port â†’ Route dÃ¹ng port Ä‘Ã³.
- Náº¿u Service cÃ³ nhiá»u port â†’ báº¡n pháº£i chá»‰ Ä‘á»‹nh --port.

*CÃ¡ch kiá»ƒm tra port service*
```
oc get service openshift-dev-deploy-cli-weather -o yaml
---
ports:
- name: 8080-tcp
  port: 8080
  targetPort: 8080

```
Service cÃ³ 8080 vÃ  8443 â†’ pháº£i chá»‰ Ä‘á»‹nh:
```
oc expose service myservice --port=8080
```

**Inner loop vs Outer loop**

![alt text](pic/7.png)

1. Inner loop vs Outer loop

- Inner loop: vÃ²ng láº·p ngáº¯n cá»§a dev â†’ code â†’ build â†’ test nhanh â†’ debug â†’ láº·p láº¡i.
  - ÄÃ¢y lÃ  pháº§n mÃ  odo há»— trá»£ máº¡nh nháº¥t (deploy code nhanh lÃªn cluster Ä‘á»ƒ test).

- Outer loop: vÃ²ng láº·p dÃ i hÆ¡n â†’ build artifact chÃ­nh thá»©c â†’ cháº¡y integration test, security test â†’ deploy vÃ o mÃ´i trÆ°á»ng prod/staging.
  - odo cÅ©ng cÃ³ thá»ƒ Ä‘á»‹nh nghÄ©a Ä‘Æ°á»£c qua devfile.yaml.

2. Devfile (devfile.yaml)

LÃ  file trung tÃ¢m, mÃ´ táº£:
- Components: container images, manifests, volumeâ€¦
- Commands: cÃ¡c bÆ°á»›c cáº§n cháº¡y (build, run, test, debug, deploy).
- Command groups: gom lá»‡nh theo nhÃ³m (`build, run, test, debug, deploy`).
Nhá» devfile, odo biáº¿t pháº£i lÃ m gÃ¬ khi báº¡n gÃµ:
- odo build
- odo run
- odo test
- odo debug
- odo deploy

Note: 
- Náº¿u muá»‘n odo <kind> cháº¡y máº·c Ä‘á»‹nh, báº¡n pháº£i báº­t isDefault: true.
- Náº¿u cÃ³ nhiá»u command cÃ¹ng kind thÃ¬ chá»‰ 1 Ä‘Æ°á»£c default, cÃ²n láº¡i pháº£i gá»i báº±ng --command <id>.

3. `odo init`
- Táº¡o file devfile.yaml ban Ä‘áº§u.
- odo sáº½ cá»‘ gáº¯ng Ä‘oÃ¡n runtime tá»« source code (vÃ­ dá»¥ tháº¥y package.json â†’ Node.js, tháº¥y requirements.txt â†’ Python).
- Náº¿u báº¡n khÃ´ng truyá»n options, nÃ³ sáº½ má»Ÿ interactive mode Ä‘á»ƒ báº¡n chá»n.
- NÃ³ cáº§n káº¿t ná»‘i tá»›i devfile registry (máº·c Ä‘á»‹nh: https://registry.devfile.io
) Ä‘á»ƒ láº¥y template phÃ¹ há»£p.

> Sau khi cÃ³ devfile.yaml, báº¡n cÃ³ thá»ƒ sá»­a Ä‘á»ƒ phÃ¹ há»£p vá»›i project.

4. Lá»‡nh `odo create project`
```
odo create project PROJECT_NAME
```
- Táº¡o má»™t OpenShift Project (namespace) má»›i trá»±c tiáº¿p tá»« odo.
- TÆ°Æ¡ng Ä‘Æ°Æ¡ng vá»›i lá»‡nh: `oc new-project PROJECT_NAME`

The following devfile provides a simplified example for outer loop development with odo. The devfile adheres to the following requirements:

- The `Dockerfile` must exist in the same directory as the `devfile.yaml`.

- The `deploy.yaml` contains the Kubernetes resources for the application.

- The parent devfile must exist.

- The odo command must be able to access Podman and an authenticated image registry.

![alt text](pic/10.png)

Example deploy.yaml
```bash
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nodejs-app
  labels:
    app: nodejs-app
spec:
...
---
apiVersion: v1
kind: Service
metadata:
  name: nodejs-app
  labels:
    app: nodejs-app
spec:
  selector:

...
---
apiVersion: route.openshift.io/v1
kind: Route
metadata:
  name: nodejs-app
spec:
  to:

```
**Container Image Renaming with odo**  
*Váº¥n Ä‘á»*
- Trong devfile.yaml báº¡n cÃ³ thá»ƒ Ä‘á»‹nh nghÄ©a image component Ä‘á»ƒ build/push image.
- Náº¿u báº¡n viáº¿t tháº³ng image name Ä‘áº§y Ä‘á»§ (vÃ­ dá»¥: quay.io/myuser/nodejs-app:1.0), thÃ¬ devfile bá»‹ gáº¯n cháº·t vá»›i registry Ä‘Ã³ â†’ khÃ³ chia sáº» cho ngÆ°á»i khÃ¡c.
- VÃ¬ váº­y, odo há»— trá»£ image renaming Ä‘á»ƒ lÃ m devfile portable (dÃ¹ng láº¡i á»Ÿ nhiá»u cluster/registry khÃ¡c nhau).

*CÃ¡ch hoáº¡t Ä‘á»™ng*

1. Khai bÃ¡o `ImageRegistry`
Báº¡n set registry máº·c Ä‘á»‹nh Ä‘á»ƒ odo push image:
```
odo preference set ImageRegistry quay.io/myuser

# syntax
[user@host ~]$ odo preference set ImageRegistry REGISTRY_URL/NAMESPACE
```
â†’ Sau Ä‘Ã³ má»i image sáº½ Ä‘Æ°á»£c push vá» quay.io/myuser/....

2. DÃ¹ng imageName tÆ°Æ¡ng Ä‘á»‘i trong devfile  
Trong `devfile.yaml`:
```
components:
  - name: relative-image
    image:
      imageName: "my-relative-image"   # chá»‰ cÃ³ tÃªn, khÃ´ng cÃ³ registry
```

ğŸ‘‰ ÄÃ¢y gá»i lÃ  relative image name.

3. Khi build/push, odo sáº½ tá»± Ä‘á»•i tÃªn image thÃ nh:
```
<ImageRegistry>/<DevfileName>-<ImageName>:<UniqueId>
```
- ImageRegistry: giÃ¡ trá»‹ báº¡n set (quay.io/myuser)
- DevfileName: tÃªn devfile (vÃ­ dá»¥ nodejs-app)
- ImageName: pháº§n báº¡n Ä‘á»‹nh nghÄ©a (my-relative-image)
- UniqueId: giÃ¡ trá»‹ random Ä‘á»ƒ trÃ¡nh trÃ¹ng

ğŸ“ VÃ­ dá»¥ cá»¥ thá»ƒ

Devfile nodejs-app cÃ³:
```
components:
  - name: relative-image
    image:
      imageName: "my-relative-image"
```

Báº¡n set:
```
odo preference set ImageRegistry quay.io/nghia
```

Khi odo deploy, image thá»±c táº¿ Ä‘Æ°á»£c push sáº½ thÃ nh:
```
quay.io/nghia/nodejs-app-my-relative-image:abc123
```
âœ… Lá»£i Ã­ch

- Devfile khÃ´ng bá»‹ hard-code registry â†’ portable, dá»… chia sáº» cho team khÃ¡c.
- odo tá»± Ä‘á»™ng Ä‘á»•i tÃªn vÃ  push image Ä‘Ãºng registry báº¡n chá»‰ Ä‘á»‹nh.
- Báº¡n cÃ³ thá»ƒ kiá»ƒm tra:
  - Trong registry (quay.io/nghia/...)
  - Hoáº·c trong resource YAML mÃ  odo apply lÃªn cluster.

**Syntax há»¯u Ã­ch cá»§a oc new-app**
```bash
oc new-app <image>                  # Táº¡o tá»« image
oc new-app --image=<image>
oc new-app <image>~<git-repo>       # S2I build tá»« source code
oc new-app -f template.yaml         # DÃ¹ng template file
oc new-app --name=myapp             # Äáº·t tÃªn app
oc new-app <image> -e VAR=VALUE     # ThÃªm biáº¿n mÃ´i trÆ°á»ng
oc new-app <image> --as-deployment-config  # DÃ¹ng DeploymentConfig thay vÃ¬ Deployment
```

**Image mÃ  oc new-app dÃ¹ng tá»« Ä‘Ã¢u ra?**

![alt text](pic/11.png)

ğŸ”¹ 2. Váº­y podman build liÃªn quan gÃ¬?
- Náº¿u báº¡n tá»± build má»™t image báº±ng podman build trÃªn mÃ¡y local thÃ¬ image Ä‘Ã³ chá»‰ náº±m trong local host.
- Äá»ƒ OpenShift dÃ¹ng Ä‘Æ°á»£c, báº¡n pháº£i push image Ä‘Ã³ lÃªn má»™t registry (vÃ­ dá»¥: quay.io, docker.io, hoáº·c internal OpenShift registry image-registry.openshift-image-registry.svc:5000).

Sau Ä‘Ã³ báº¡n má»›i dÃ¹ng:
```
oc new-app <registry>/<namespace>/<image>:<tag>
```

Máº·c Ä‘á»‹nh `oc new-app` chá»‰ giÃºp báº¡n deploy nhanh Ä‘á»ƒ test, cÃ²n náº¿u muá»‘n tÃ¹y chá»‰nh chi tiáº¿t thÃ¬ cÃ³ vÃ i cÃ¡ch:

![alt text](pic/12.png)

**TÃ¹y chá»‰nh**

- TÃ¹y chá»‰nh ngay tá»« lá»‡nh `oc new-app`
```bash
# Example
oc scale deployment myapp --replicas=3
oc expose deployment myapp --port=8080 --target-port=8080
```
- Chá»‰nh sau khi resource Ä‘Æ°á»£c táº¡o
```bash
# Xuáº¥t YAML ra file Ä‘á»ƒ chá»‰nh:
oc get deployment myapp -o yaml > myapp-deploy.yaml

# -> Sá»­a YAML (replicas, resource limits, volume, env, liveness probeâ€¦)

# Apply láº¡i:
oc apply -f myapp-deploy.yaml
```
3. DÃ¹ng workflow hiá»‡n Ä‘áº¡i (recommended)  
Thay vÃ¬ `oc new-app` (quick & simple), báº¡n cÃ³ thá»ƒ:
- Viáº¿t sáºµn Deployment + Service YAML â†’ `oc apply -f`.
- Hoáº·c dÃ¹ng odo vá»›i devfile.yaml náº¿u phÃ¡t triá»ƒn app.
- Hoáº·c dÃ¹ng BuildConfig + ImageStream náº¿u muá»‘n workflow CI/CD theo kiá»ƒu OpenShift truyá»n thá»‘ng.

---
# Chapter 3.  Building and Publishing Container Images

Red Hat Universal Base Images
When defining custom container images, Red Hat recommends the use of Red Hat Universal Base Images (UBI) as the base container images for your applications. UBI images are certified, tested, and regularly maintained images that Red Hat provides at no cost.

UBI images also provide the following major benefits:

**Universal**  
UBI images are designed to be used as the base images for developing container-based applications.

**Robust**  
UBI images are based on Red Hat Enterprise Linux (RHEL). This brings characteristics such as stability and vulnerability management to your base container images.

**Standard**  
UBI images are compliant with the Open Container Initiative (OCI).

**Extensible**  
UBI images provide package managers and other tools for installing additional software.

**OpenShift-optimized**  
UBI images are tailored to work well on Red Hat OpenShift.

**Redistributable**  
The UBI End-User Licensing Agreement (EULA) permits free distribution of the applications that you build on top of UBI images.

Red Hat provides four types of UBI images, designed to cover most use cases.

Image type	|Image name	|Uses
---|---|---
Standard	|ubi|	For most applications and use cases
Init	|ubi-init|	For containers that run multiple systemd services
Minimal	|ubi-minimal	|Smaller image for applications that manage their own dependencies and depend on fewer OS components
Micro	|ubi-micro	|Smallest image for optimized memory-footprint use cases; for applications that use almost no OS components

Runtime UBI Images For Developers
Red Hat provides UBI images for the following runtime languages:
- OpenJDK
- Node.js
- Python
- PHP
- .NET
- Go
- Ruby
 
Optimize Containerfiles for OpenShift  
Format
```
registry.access.redhat.com/NAMESPACE/NAME[:TAG]
```
Example 
```
FROM registry.access.redhat.com/ubi10/nodejs-22-minimal:10.0
```
**Ensure That Your Containers Handle Interruption Signals**

ğŸ”¹ 1. CÆ¡ cháº¿ shutdown máº·c Ä‘á»‹nh

Khi báº¡n xÃ³a Pod hoáº·c rollout Deployment má»›i, OpenShift sáº½:
- Gá»­i tÃ­n hiá»‡u SIGTERM Ä‘áº¿n process PID 1 trong container.
- Container/app cá»§a báº¡n cÃ³ trÃ¡ch nhiá»‡m ngáº¯t káº¿t ná»‘i, Ä‘Ã³ng resource, lÆ°u dataâ€¦.
- Náº¿u trong thá»i gian terminationGracePeriodSeconds (máº·c Ä‘á»‹nh 30s) app khÃ´ng táº¯t â†’ OpenShift gá»­i SIGKILL (kill ngay láº­p tá»©c).

ğŸ‘‰ Do Ä‘Ã³, á»©ng dá»¥ng pháº£i biáº¿t cÃ¡ch handle SIGTERM Ä‘á»ƒ shutdown â€œÃªm Ä‘áº¹pâ€ (graceful shutdown).

ğŸ”¹ 2. TrÆ°á»ng há»£p á»©ng dá»¥ng khÃ´ng handle SIGTERM

VÃ­ dá»¥:
- Má»™t app Java cháº¡y báº±ng java -jar example.jar.
- Náº¿u báº¡n khÃ´ng trap SIGTERM, khi Pod bá»‹ kill â†’ Java process sáº½ táº¯t ngay â†’ cÃ³ thá»ƒ máº¥t data, chÆ°a commit transaction, connection bá»‹ cáº¯t Ä‘á»™t ngá»™t.

CÃ¡ch xá»­ lÃ½:
- Viáº¿t entrypoint script (nhÆ° vÃ­ dá»¥ trong bÃ i): trap tÃ­n hiá»‡u SIGTERM vÃ  forward cho app, rá»“i wait cho app shutdown.
```
trap graceful_shutdown SIGTERM
java -jar example.jar &
java_pid=$!
wait "$java_pid"
```

á» Ä‘Ã¢y:

- trap graceful_shutdown SIGTERM: khi nháº­n SIGTERM â†’ gá»i hÃ m graceful_shutdown.

- HÃ m nÃ y gá»­i SIGTERM Ä‘áº¿n process Java (kill -SIGTERM $java_pid) vÃ  Ä‘á»£i nÃ³ shutdown.

ğŸ”¹ 3. Khi app khÃ´ng sá»­a code Ä‘Æ°á»£c

Náº¿u á»©ng dá»¥ng khÃ´ng cÃ³ cÆ¡ cháº¿ nháº­n SIGTERM (hoáº·c báº¡n khÃ´ng thá»ƒ thay Ä‘á»•i entrypoint), báº¡n cÃ³ thá»ƒ dÃ¹ng Pod lifecycle hook:
```
lifecycle:
  preStop:
    httpGet:
      path: /shutdown
      port: 8080
```

- Khi Pod chuáº©n bá»‹ bá»‹ xÃ³a, kubelet sáº½ gá»i HTTP GET vÃ o /shutdown.

- á»¨ng dá»¥ng sáº½ nháº­n request nÃ y vÃ  thá»±c hiá»‡n cleanup (Ä‘Ã³ng káº¿t ná»‘i, flush cache, lÆ°u tráº¡ng thÃ¡i, v.v).

- Sau Ä‘Ã³ má»›i nháº­n SIGTERM Ä‘á»ƒ táº¯t háº³n.

ğŸ”¹ 4. Náº¿u app váº«n khÃ´ng táº¯t?

- Sau khi háº¿t thá»i gian terminationGracePeriodSeconds, kubelet/OpenShift gá»­i SIGKILL â†’ process bá»‹ kill ngay láº­p tá»©c.

- LÃºc nÃ y khÃ´ng cÃ³ cÆ¡ há»™i cleanup â†’ nguy cÆ¡ máº¥t dá»¯ liá»‡u hoáº·c lá»—i.

âœ… TÃ³m láº¡i

- OpenShift/K8s luÃ´n gá»­i SIGTERM trÆ°á»›c Ä‘á»ƒ cho app tá»± shutdown Ãªm Ä‘áº¹p.

- App nÃªn handle SIGTERM (qua entrypoint script).

- Náº¿u khÃ´ng thá»ƒ â†’ dÃ¹ng preStop hook Ä‘á»ƒ app cleanup trÆ°á»›c khi bá»‹ kill.

- Náº¿u app váº«n khÃ´ng shutdown ká»‹p â†’ cuá»‘i cÃ¹ng bá»‹ SIGKILL (force kill).

![alt text](pic/13.png)

**Reduce Image Size**

1. Giáº£m sá»‘ lÆ°á»£ng RUN
- Káº¿t há»£p nhiá»u lá»‡nh thÃ nh 1 RUN Ä‘á»ƒ giáº£m sá»‘ layer.

2. Giáº£m build context
- DÃ¹ng .dockerignore / .containerignore Ä‘á»ƒ loáº¡i bá» file/thÆ° má»¥c khÃ´ng cáº§n thiáº¿t.

3. Multistage build
- Stage Ä‘áº§u: build app báº±ng image Ä‘áº§y Ä‘á»§ (vÃ­ dá»¥: ubi9/nodejs-22).
- Stage cuá»‘i: copy artifact sang image minimal/runtime (ubi9/nodejs-22-minimal).
- â†’ Káº¿t quáº£: image nhá» gá»n, chá»‰ chá»©a runtime + app.

4. DÃ¹ng minimal image

- Node.js: nodejs-22-minimal.
- OpenJDK: ubi9/openjdk-21-runtime.

5. LABEL
- DÃ¹ng LABEL Ä‘á»ƒ khai bÃ¡o metadata.
- VÃ­ dá»¥: LABEL io.openshift.min-cpu 2 â†’ UI OpenShift cáº£nh bÃ¡o cáº§n Ã­t nháº¥t 2 CPU.

6. WORKDIR
- LuÃ´n dÃ¹ng WORKDIR vá»›i absolute path thay vÃ¬ nhiá»u láº§n cd trong RUN.
7. ENV vÃ  ARG
- DÃ¹ng ENV Ä‘á»ƒ cáº¥u hÃ¬nh Ä‘Æ°á»ng dáº«n, version, PATHâ€¦
- DÃ¹ng ARG cho biáº¿n build-time â†’ táº¡o image tÃ¡i sá»­ dá»¥ng Ä‘Æ°á»£c.
8. VOLUME
- Khai bÃ¡o rÃµ VOLUME Ä‘á»ƒ ngÆ°á»i dÃ¹ng biáº¿t mount dá»¯ liá»‡u.
- Náº¿u khÃ´ng mount, OpenShift sáº½ tá»± gáº¯n EmptyDir (ephemeral).

9. EXPOSE
- Chá»‰ expose port >1024 (khÃ´ng cáº§n quyá»n root).
- oc new-app sáº½ tá»± táº¡o Deployment + Service theo port Ä‘Æ°á»£c EXPOSE.
- Web console cÅ©ng nháº­n diá»‡n cÃ¡c port nÃ y Ä‘á»ƒ cáº¥u hÃ¬nh service.

ğŸ‘‰ TÃ³m gá»n láº¡i:
- Multistage + minimal UBI image â†’ giáº£m size.
- LABEL, ENV, VOLUME, EXPOSE â†’ giÃºp OpenShift hiá»ƒu rÃµ image, há»— trá»£ cáº¥u hÃ¬nh dá»… dÃ ng.
- WORKDIR vÃ  Ã­t RUN â†’ lÃ m Containerfile gá»n, dá»… maintain.

**Build and Push Images with Podman**  
You can use a tool such as Podman to build a container image locally and push the image to a container registry.

> Note
You can also use OpenShift to build your container images. The OpenShift build capabilities, such as Source-to-Image (S2I) and Docker builds, are covered later in this course. OpenShift also provides the OpenShift Builds framework, which is based on the Shipwright project.

Use the podman build command to create a container image from a Containerfile, as the following example shows:
```
[user@host ~]$ podman build CONTEXT_DIR -t IMAGE
```
The preceding command creates a local container image by using the Containerfile or Dockerfile at the CONTEXT_DIR directory. The produced image is called IMAGE. After building the container image locally, push the image to a container registry by using the podman push command.
```
[user@host ~]$ podman push IMAGE
```
> Note
You must be logged in to the registry to push images. You can log in with the podman login command.

After the image is published in the container registry, you can deploy the image by using any of the methods that Red Hat OpenShift provides, such as the web console, or the oc and odo CLIs.

## Guided Exercise: Building Container Images for Red Hat OpenShift


```bash
oc new-app \
--name greetings \
--image=registry.ocp4.example.com:8443/developer/images-ubi-greetings:1.0.0
```
File Containerfile
![alt text](pic/15.png)

3 error khi `oc new-app`  
![alt text](pic/14.png)

**Fix 1**  
Remove the USER instruction from the Containerfile. The file must look as follows:
```
FROM registry.ocp4.example.com:8443/ubi10/nodejs-22-minimal:10.0

ENV PORT=80
EXPOSE ${PORT}

ADD . $HOME

RUN npm ci --omit=dev && rm -rf .npm

CMD npm start
```

**Fix 2**  
Trong Linux (vÃ  cÅ©ng Ä‘Ãºng trong container), cÃ³ quy táº¯c:
- Port < 1024 = privileged ports â†’ chá»‰ process cháº¡y báº±ng root user má»›i bind Ä‘Æ°á»£c.
- Port â‰¥ 1024 = unprivileged ports â†’ báº¥t ká»³ non-root user nÃ o cÅ©ng bind Ä‘Æ°á»£c.

VÃ¬ sao lá»—i xáº£y ra?
- á»¨ng dá»¥ng cá»§a báº¡n cá»‘ gáº¯ng láº¯ng nghe trÃªn cá»•ng 80:
```
Error: listen EACCES: permission denied 0.0.0.0:80
```
- NhÆ°ng trong OpenShift, container cháº¡y vá»›i má»™t UID ngáº«u nhiÃªn, khÃ´ng pháº£i root.
- Do Ä‘Ã³, process Ä‘Ã³ khÃ´ng thá»ƒ má»Ÿ Ä‘Æ°á»£c cá»•ng 80 â†’ bÃ¡o lá»—i EACCES.

**Fix 3**  
Fix the permissions of the /var/cache directory. In the Containerfile, add a RUN instruction that runs as the root user and ensures that the group assigned to the /var/cache directory is the root group (0). Then, it should grant the root group the same permissions as the user that owns this directory. Finally, it should restore 1001 as the user ID that runs the application.
```bash
...omitted...
RUN npm ci --omit=dev && rm -rf .npm

USER root
RUN chgrp -R 0 /var/cache && \
    chmod -R g=u /var/cache
USER 1001

CMD npm start
```
-> rebuild image, run -rm , podman push -> oc new-app -> expose > get url

## Using External Registries in Red Hat OpenShift
There are many kinds of container registries:

**Public registries**  
Registries that allow anyone to consume container images directly from the internet without any authentication, such as Docker Hub, Quay.io, or the Red Hat Registry.

**Private registries**  
Registries that are available only to selected consumers and usually require authentication. The Red Hat terms-based registry is an example of a private container registry.

**Enterprise registries**  
Registries that your organization manages. Such registries are usually available only to the organization's employees.

**OpenShift internal registries**  
A registry server managed internally by an OpenShift cluster to store container images.

These kinds of registries are not mutually exclusive: a registry can be, at the same time, both public and private.

**Creating Registry Credentials in OpenShift**  
1. You can use the oc create command to create a secret, for example:
```
[user@host ~]$ oc create secret generic example-secret \
--from-literal=user=developer --namespace=example-ns
secret/example-secret created
```
2. You can use the OpenShift console to create secrets. In the Developer perspective, click Secrets. Select a project, click Create, and then select the secret type that you want to create.

![alt text](pic/17.png)

3. Kubernetes provides the docker-registry secret type to store credentials for authentication with the container registry.

```bash
[user@host ~]$ oc create secret docker-registry SECRET_NAME \
--docker-server REGISTRY_URL \
--docker-username USER \
--docker-password PASSWORD \
--docker-email=EMAIL
secret/SECRET_NAME created
```
4. Táº¡o trá»±c tiáº¿p trong Web Console (GUI)
- VÃ o Workloads â†’ Secrets â†’ Create â†’ Image pull secret.
- Nháº­p server URL, user, password, email.

![alt text](pic/16.png)

You can also create the secret from existing credentials. For example, if you logged in to the private registry with Podman, then you have existing credentials in the `${XDG_RUNTIME_DIR}/containers/auth.json` file. Because the auth.json file uses the same structure as the .dockerconfigjson file, you can create the secret by using the `auth.json` file.
```
[user@host ~]$ oc create secret generic SECRET_NAME \
--from-file .dockerconfigjson=${XDG_RUNTIME_DIR}/containers/auth.json \
--type kubernetes.io/dockerconfigjson
```
You can also upload the auth.json file in the OpenShift console when creating the secret.

**Configuring OpenShift to Use the Registry Credentials**  
You can configure OpenShift to use custom credentials by using the `spec.imagePullSecrets` Pod property, for example:
```
apiVersion: v1
kind: Pod
metadata:
  name: example-pod
spec:
  containers:
  - name: example-container
    image: REGISTRY_URL
  imagePullSecrets:
  - name: SECRET_NAME
```
Consequently, you can use the property for controllers, such as the Deployment objects:
```
apiVersion: apps/v1
kind: Deployment
metadata:
  name: example-deployment
spec:
  replicas: 3
  selector:
    matchLabels:
      app: my-app
  template:
    metadata:
      labels:
        app: my-app
    spec:
      containers:
        - name: example-container
          image: REGISTRY_URL
      imagePullSecrets:
        - name: SECRET_NAME
```

**Linking Registry Credentials to Service Accounts**

Instead of manually assigning the credentials to pods, you can configure OpenShift to assign the credentials to pods automatically by using service accounts. A service account provides an identity for pods. Pods use the default service account unless you configure a different service account.

Use the oc secrets link command to connect a secret with a service account, for example:
```
[user@host ~]$ oc secrets link --for=pull default SECRET_NAME
no output expected
```
Hoáº·c cho build config:
```
oc secrets link builder my-registry-secret --for=pull
```
The preceding command creates a new entry in the service account imagePullSecrets field:
```
apiVersion: v1
kind: ServiceAccount
metadata:
  name: default
imagePullSecrets:
- name: SECRET_NAME
```
When you create a pod that uses the default service account, it inherits the imagePullSecrets field without you explicitly specifying the field in the pod definition.

This means that every pod that uses the default service account is authorized with the registry credentials in your secret.  
Giai thich:
- Báº¡n Ä‘Ã£ táº¡o secret trÆ°á»›c Ä‘Ã³ (chá»©a thÃ´ng tin registry account).
- CÃ¢u lá»‡nh nÃ y sáº½ gáº¯n secret vÃ o ServiceAccount default cá»§a project hiá»‡n táº¡i.
- Ká»ƒ tá»« lÃºc Ä‘Ã³, báº¥t ká»³ Pod nÃ o trong project cháº¡y vá»›i SA default sáº½ tá»± Ä‘á»™ng dÃ¹ng secret nÃ y khi pull image tá»« registry private.

-> 2 cÃ¡ch chÃ­nh Ä‘á»ƒ sá»­ dá»¥ng secret Ä‘Ã³ trong OpenShift/Kubernetes:
- CÃ¡ch 1: Khai bÃ¡o trá»±c tiáº¿p trong workload (spec.imagePullSecrets)
- CÃ¡ch 2: Gáº¯n secret vÃ o ServiceAccount (oc secrets link)

NguyÃªn táº¯c
- Secret lÃ  resource â€œnamespace-scopedâ€ â†’ chá»‰ tá»“n táº¡i trong project (namespace) nÃ o nÃ³ Ä‘Æ°á»£c táº¡o.
- Do Ä‘Ã³:
  - Secret trong project A khÃ´ng thá»ƒ dÃ¹ng trá»±c tiáº¿p á»Ÿ project B.
  - Náº¿u project B cÅ©ng cáº§n dÃ¹ng â†’ báº¡n pháº£i táº¡o láº¡i secret trong project B (cÃ³ thá»ƒ cÃ¹ng thÃ´ng tin).

CÃ³ thá»ƒ dung ServiceAccount khÃ¡c thay vÃ¬ default
```
oc secrets link custom-sa my-registry-secret --for=pull

# 
apiVersion: apps/v1
kind: Deployment
metadata:
  name: myapp
spec:
  replicas: 1
  selector:
    matchLabels:
      app: myapp
  template:
    metadata:
      labels:
        app: myapp
    spec:
      serviceAccountName: custom-sa   # dÃ¹ng SA nÃ y thay vÃ¬ default
      containers:
      - name: myapp
        image: quay.io/myuser/private-app:1.0

```

*Search for the failed event messages that the application emits.*
```
[student@workstation ~]$ oc get event --field-selector type=Warning \
-o jsonpath='{range .items[]}{.message}{"\n"}{end}'
Failed to pull image "registry.ocp4.example.com:8443/redhattraining/hello-world-nginx:latest": ... invalid username/password: unauthorized: ...
```

![alt text](pic/18.png)

Docs: https://docs.redhat.com/en/documentation/openshift_container_platform/4.8/html/authentication_and_authorization/understanding-and-creating-service-accounts


Fix error  "Robot Accounts"

![alt text](pic/19.png)

7. Delete the robot account from the internal registry.

On the Robot Accounts page, click the gear icon of the developer+ocprobot account, and then click Delete Robot developer+ocprobot. Click Ok to complete the process.

## 3.5 Creating Image Streams
Image streams have the following benefits:
- They provide a level of indirection to the container image that OpenShift runs.
- They allow for rolling back to a previous container version without updating the image registry.
- They enable build and deployment automations when an image stream tag gets updated.
- They enable the caching of images from external image registries.
- You can use role-based access control (RBAC) on the image stream object to secure access to container images.

Váº¥n Ä‘á»: má»—i láº§n Ä‘á»•i image â†’ báº¡n pháº£i can thiá»‡p thá»§ cÃ´ng.
Trong khi Ä‘Ã³, náº¿u dÃ¹ng ImageStream + ImageChange trigger thÃ¬ chá»‰ cáº§n cáº­p nháº­t oc tag hoáº·c build má»›i   
â†’ OpenShift tá»± rollout cho báº¡n.


![alt text](pic/20.png)

![alt text](pic/21.png)

**Managing Image Streams and Tags**

For example, the following command imports a my-app-stream container image from an external container registry and periodically checks for updates:
```
[user@host ~]$ oc import-image myimagestream --confirm --scheduled=true \
--from example.com/example-repo/my-app-image
```
To create one image stream tag resource for each container image tag that exists in the source registry server, add the --all option to the oc import-image command. The following command creates or updates all image stream tags for new tags on the source registry server:
```
[user@host ~]$ oc import-image myimagestream --confirm --all \
--from registry/myorg/myimage
```
Running the oc import-image command on an existing image stream updates one of its current image stream tags to the current image IDs on the source registry server, such as in the following command:
```
[user@host ~]$ oc import-image myimagestream[:tag]
```
Exert finer control over an image stream tag by using the oc tag command. This enables you to associate an image stream tag with the following:
- A different registry than the one in its image stream
- A different container image name and tag
- An image ID that might not be the one currently associated with the image tag on the registry server
- An alias for the image stream tag

For example, to update the latest image stream tag to point to a different tag you can run the following command:
```
[user@host ~]$ oc tag myimagestream:tag myimagestream:latest
```
**Creating Image Streams From Private Registries**  
The following example commands use Podman to log in to a private registry, create a secret to store the access token, and create an image stream that points to the private registry:
```
[user@host ~]$ podman login -u myuser registry.example.com
[user@host ~]$ oc create secret generic regtoken \
--from-file .dockerconfigjson=${XDG_RUNTIME_DIR}/containers/auth.json \
--type kubernetes.io/dockerconfigjson
[user@host ~]$ oc import-image myimagestream --confirm \
--from registry.example.com/myorg/myimage
```
After you create an image stream, you can use it to deploy an application by using the oc new-app command and by using the `-i` option to specify the image stream.

By default, an image stream resource is only available to create applications or builds in the same project.

**Using Image Streams with Kubernetes Resources**

**Sharing an Image Stream Between Multiple Projects**

ğŸ“Œ 1. Táº¡o vÃ  quáº£n lÃ½ ImageStream

Táº¡o IS rá»—ng:
```
oc create is myapp
```

Xem danh sÃ¡ch IS trong project:
```
oc get is
```

Xem chi tiáº¿t 1 IS:
```
oc describe is myapp
```

XoÃ¡ IS:
```
oc delete is myapp
```
ğŸ“Œ 2. Import image tá»« registry vÃ o IS

Import má»™t image cá»¥ thá»ƒ:
```
oc import-image myapp:1.0 --from=quay.io/example/myapp:1.0 --confirm
```

Import táº¥t cáº£ tag tá»« má»™t repo:
```
oc import-image myapp --from=quay.io/example/myapp --all --confirm
```

Import cÃ³ lá»‹ch trÃ¬nh (scheduled):
```
oc import-image myapp --from=quay.io/example/myapp --confirm --scheduled=true
```

Cáº­p nháº­t tag hiá»‡n cÃ³:
```
oc import-image myapp:latest
```
ğŸ“Œ 3. Quáº£n lÃ½ tag cá»§a IS

Táº¡o alias giá»¯a cÃ¡c tag:
```
oc tag myapp:1.0 myapp:latest
```

Copy tá»« IS khÃ¡c:
```
oc tag otheris:2.0 myapp:dev
```

GÃ¡n IS tag vá»›i image ngoÃ i registry:
```
oc tag quay.io/example/myapp:3.0 myapp:stable
```
ğŸ“Œ 4. LiÃªn quan tá»›i á»©ng dá»¥ng

DÃ¹ng IS Ä‘á»ƒ táº¡o app (DeploymentConfig):
```
oc new-app -i myapp:latest --name=myapp
```

Xem Pod nÃ o Ä‘ang dÃ¹ng IS:
```
oc describe is myapp
```

(pháº§n â€œImage Stream Tag Historyâ€ sáº½ show DeploymentConfig, Builds nÃ o Ä‘ang dÃ¹ng tag Ä‘Ã³).

ğŸ“Œ 5. BuildConfig vÃ  IS

Build output vá» IS:
```
oc new-build --binary --name=myapp --to=myapp:1.0
oc start-build myapp --from-dir=. --follow
```

ğŸ‘‰ TÃ³m gá»n:

- oc create is / oc delete is â†’ quáº£n lÃ½ ImageStream.
- oc import-image â†’ kÃ©o image tá»« registry vÃ o IS.
- oc tag â†’ quáº£n lÃ½ tag trong IS.
- oc new-app -i â†’ deploy app tá»« IS (DeploymentConfig + trigger).
- oc describe is â†’ theo dÃµi ai Ä‘ang dÃ¹ng IS.

---
# Chapter 4.  Managing Red Hat OpenShift Builds
ğŸ”¹ OpenShift Build Process (CÃ¡ch OpenShift build image)
1. CÃ¡c thÃ nh pháº§n chÃ­nh khi build

- Trigger â†’ cÃ¡i gÃ¬ khá»Ÿi cháº¡y build (git commit, webhook, thay Ä‘á»•i image).
- Strategy â†’ build theo cÃ¡ch nÃ o (Source-to-Image (S2I), Dockerfile/Buildah, Custom).
- Input sources â†’ code, binary, hoáº·c Dockerfile.
- Output â†’ image Ä‘Æ°á»£c push vÃ o registry.

ğŸ‘‰ OpenShift cung cáº¥p 2 cÃ¡ch build chÃ­nh:

- BuildConfig (cá»• Ä‘iá»ƒn): tÃ i nguyÃªn gá»‘c cá»§a OpenShift, khai bÃ¡o YAML rá»“i build.
- Shipwright (má»›i, Kubernetes-native): dá»±a trÃªn upstream project, linh hoáº¡t, há»— trá»£ nhiá»u tool (S2I, Buildah, Buildpacks).

2. Shipwright (Builds for OpenShift)
- Kubernetes-native: dÃ¹ng CRD nhÆ° cÃ¡c resource khÃ¡c.
- Linh hoáº¡t: há»— trá»£ nhiá»u chiáº¿n lÆ°á»£c build.
- Dá»… dÃ¹ng: cÃ³ CLI shp, tÃ­ch há»£p trong Web Console.

CÃ¡c CR quan trá»ng:
- Build: Ä‘á»‹nh nghÄ©a cÃ¡i gÃ¬ cáº§n build (nguá»“n code, chiáº¿n lÆ°á»£c, output image).
- BuildStrategy / ClusterBuildStrategy: mÃ´ táº£ cÃ¡ch build (vÃ­ dá»¥ S2I, Buildah, Buildpacks).
- BuildRun: khi cháº¡y build, sáº½ táº¡o ra má»™t Pod thá»±c hiá»‡n build.

3. VÃ­ dá»¥ vá»›i Buildah

Build resource (khai bÃ¡o build):
```bash
apiVersion: shipwright.io/v1beta1
kind: Build
metadata:
  name: buildah-golang-build
spec:
  source:                 # code nguá»“n
    type: Git
    git:
      url: https://github.com/shipwright-io/sample-go
    contextDir: docker-build
  strategy:               # chiáº¿n lÆ°á»£c build
    name: buildah
    kind: ClusterBuildStrategy
  paramValues:            # tham sá»‘ build
  - name: dockerfile
    value: Dockerfile
  output:                 # image output
    image: image-registry.openshift-image-registry.svc:5000/buildah-example/sample-go-app
```

ğŸ‘‰ Giáº£i thÃ­ch:
- source: láº¥y code tá»« GitHub.
- strategy: dÃ¹ng Buildah.
- paramValues: chá»‰ ra Dockerfile.
- output: image sáº½ Ä‘Æ°á»£c push vÃ o internal registry.

BuildRun resource (cháº¡y build):
```bash
apiVersion: shipwright.io/v1beta1
kind: BuildRun
metadata:
  name: buildah-golang-buildrun
spec:
  build:
    name: buildah-golang-build
```

ğŸ‘‰ BuildRun chá»‰ cáº§n tham chiáº¿u Ä‘áº¿n Build Ä‘Ã£ Ä‘á»‹nh nghÄ©a â†’ Shipwright táº¡o Pod Ä‘á»ƒ build.

4. CLI shp (thay cho YAML dÃ i dÃ²ng)

Táº¡o Build:
```bash
shp build create buildah-golang-build \
  --source-url="https://github.com/redhat-openshift-builds/samples" \
  --source-context-dir="buildah-build" \
  --strategy-name="buildah" \
  --dockerfile="Dockerfile" \
  --output-image="image-registry.openshift-image-registry.svc:5000/buildah-example/go-app"
```

Cháº¡y build:
```
shp build run buildah-golang-build --follow
```
âœ… TÃ³m táº¯t 

- BuildConfig: kiá»ƒu cÅ©, thuáº§n OpenShift.
- Shipwright (Builds for OpenShift): kiá»ƒu má»›i, Kubernetes-native, linh hoáº¡t hÆ¡n.
- Build = Ä‘á»‹nh nghÄ©a build, BuildStrategy = cÃ¡ch build, BuildRun = cháº¡y build.
- VÃ­ dá»¥: láº¥y code tá»« GitHub â†’ dÃ¹ng Buildah vá»›i Dockerfile â†’ output thÃ nh image trong registry.

**Builds using BuildConfig**

![alt text](pic/22.png)

**The S2I Build Workflow**

![alt text](pic/23.png)

âœ… TÃ³m táº¯t

- Builder image: image Ä‘áº·c biá»‡t Ä‘á»ƒ build app tá»« source â†’ output image.
- S2I scripts: assemble (cÃ¡ch build), run (cÃ¡ch cháº¡y).
- CÃ³ 2 cÃ¡ch dÃ¹ng:
  - Táº¡o builder image chá»©a sáºµn script.
  - Override báº±ng .s2i/bin/ trong repo â†’ nhanh hÆ¡n, khÃ´ng cáº§n rebuild builder image.

## 4.3 Managing Application Builds
Create a Build Configuration
There are two ways to create a build configuration using the oc CLI: `oc new-app` and `oc new-build`.




## 4.5 Triggering Builds

## 4.7 Customizing an Existing S2I Base Image

---
# Chapter 5.  Managing Red Hat OpenShift Deployments

1. KhÃ¡i niá»‡m chung
- Deployment trong OpenShift tá»± Ä‘á»™ng hÃ³a quÃ¡ trÃ¬nh cáº­p nháº­t á»©ng dá»¥ng.
- Má»¥c tiÃªu: giáº£m downtime, tÄƒng tÃ­nh á»•n Ä‘á»‹nh, há»— trá»£ phÃ¡t hÃ nh liÃªn tá»¥c.
- YÃªu cáº§u app cáº§n tuÃ¢n thá»§ best practices:
  - Xá»­ lÃ½ tÃ­n hiá»‡u SIGTERM Ä‘á»ƒ táº¯t graceful.
  - Health/Readiness probe Ä‘á»ƒ router chá»‰ gá»­i request Ä‘áº¿n pod khá»e.

2. Deployment Resource
- LÃ  Kubernetes-native (dÃ¹ng ReplicaSet).
- TÃ­nh nÄƒng: rollout theo config, scale, pause/resume rollout.
- DeploymentConfig (kiá»ƒu cÅ©) Ä‘Ã£ bá»‹ deprecated.

3. Chiáº¿n lÆ°á»£c Deployment
- CÃ³ 2 nhÃ³m:  
a. Dá»±a trÃªn Deployment Resource
- Rolling (RollingUpdate)
  - Máº·c Ä‘á»‹nh.
  - Tá»«ng bÆ°á»›c thay pod cÅ© báº±ng pod má»›i.
  - KhÃ´ng downtime.
  - DÃ¹ng khi app cháº¡y song song Ä‘Æ°á»£c nhiá»u version.
- Recreate
  - XÃ³a toÃ n bá»™ pod cÅ© â†’ cháº¡y pod má»›i.
  - CÃ³ downtime.
  - DÃ¹ng khi app khÃ´ng há»— trá»£ cháº¡y song song, hoáº·c sá»­ dá»¥ng PVC vá»›i RWO / RWOP.

b. Dá»±a trÃªn Router

1. Blue-Green
  - 2 mÃ´i trÆ°á»ng (Blue â€“ má»›i, Green â€“ cÅ©) cháº¡y song song.
  - Route trá» vÃ o version nÃ o thÃ¬ user dÃ¹ng version Ä‘Ã³.
  - Dá»… rollback.

2. A/B
- Route chia traffic theo tá»· lá»‡ (vÃ­ dá»¥ 10% Blue, 90% Green).
- DÃ¹ng Ä‘á»ƒ test dáº§n, tÄƒng traffic tá»« tá»« cho version má»›i.

## 5.3 Managing Application Deployments
`oc rollout`  
The oc rollout command provides the cancel, pause, undo, retry, and more options for your deployments.
1. Xem tráº¡ng thÃ¡i rollout
```
oc rollout status deployment example-deployment
```
- DÃ¹ng Ä‘á»ƒ theo dÃµi tiáº¿n trÃ¬nh triá»ƒn khai (xem pod cÅ© Ä‘Ã£ xoÃ¡ chÆ°a, pod má»›i Ä‘Ã£ cháº¡y chÆ°a).
- Káº¿t quáº£ vÃ­ dá»¥:
  - Waiting for deployment ... rollout to finish... â†’ Ä‘ang triá»ƒn khai.
  - successfully rolled out â†’ triá»ƒn khai thÃ nh cÃ´ng.

2. Rollback (quay láº¡i version trÆ°á»›c)
```
oc rollout undo deployment example-deployment
```
- Náº¿u version má»›i bá»‹ lá»—i, quay láº¡i version cÅ© ngay.
- Giá»‘ng nhÆ° Ctrl+Z cho Deployment ğŸ˜„.

3. Pause (táº¡m dá»«ng rollout)
```
oc rollout pause deployment example-deployment
```
- DÃ¹ng khi báº¡n muá»‘n táº¡m dá»«ng triá»ƒn khai tá»± Ä‘á»™ng (vÃ­ dá»¥: báº¡n Ä‘ang sá»­a nhiá»u config).
- LÃºc nÃ y thay Ä‘á»•i khÃ´ng Ã¡p dá»¥ng ngay.

4. Resume (tiáº¿p tá»¥c rollout)
```
oc rollout resume deployment example-deployment
```
- Sau khi chá»‰nh sá»­a xong, dÃ¹ng lá»‡nh nÃ y Ä‘á»ƒ cho rollout tiáº¿p tá»¥c.

`oc scale`  
The oc scale command scales the number of replicas for a given deployment:
```
[user@host ~]$ oc scale deployment example-deployment --replicas=3
deployment.apps/example-deployment scaled
```

**Create Secrets and Configuration Maps**
Similarly to secrets, you can create configuration maps by using the oc create command:
```
[user@host ~]$ oc create configmap example-cm \
--from-literal key1=value1 \
--from-literal key2=value2
configmap/example-cm created
```
The previous command creates the following YAML object:
```
kind: ConfigMap
metadata:
    name: example-cm
apiVersion: v1
data:
    key1: value1
    key2: value2
```
You can also create configuration maps from a file or a directory:
```
[user@host ~]$ oc create configmap example-cm \
--from-file=redis.conf
configmap/example-cm created
```
The preceding example creates a configuration map with the redis.conf key and the contents of the file as its value. Developers might also rename the key, such as:
```bash
[user@host ~]$ oc create configmap example-cm \
--from-file=primary=/etc/redis/redis.conf \
--from-file=replica=replica-redis.conf
configmap/example-cm created
```
The preceding example creates a configuration map with the following keys:
- The primary key with the contents of the local /etc/redis/redis.conf file.
- The replica key with the contents of the local ./replica-redis.conf file.

Finally, similarly to secrets, you can use the OpenShift web console to create configuration maps. In the developer perspective, click ConfigMaps, select your project, and click Create ConfigMap.

**Manage Secrets and Configuration Maps**  
*View resources*  
To view details of a resource, use the oc get command:
```
[user@host ~]$ oc get secret mysecret -o yaml
...output omitted...
```
The -o yaml parameter displays the resource in the YAML language  
*Edit resources*  
To edit a resource, use the oc edit command:
```
[user@host ~]$ oc edit configmap my-cm
...output omitted...
```
Alternatively, you can edit resources in the OpenShift web console.  

*Patch resources*  
Patching a resource refers to updating the resource by applying a set of changes rather than interactively. This is useful, for example, in scripts. Use the oc patch command to patch a resource, for example:
```
[user@host ~]$ oc patch configmap/my-cm \
--patch '{"data":{"key1":"newvalue1"}}'
configmap/my-cm patched
```
The preceding command changes the .data.key1 key to the newvalue1 value.

The preceding commands work on any OpenShift resource. However, to edit secrets, you must use values in base64 encoding. You can encode any string by using the base64 command, for example:
```bash
[user@host ~]$ echo -n 'hunter3' | base64
aHVudGVyMw==

# decode
[user@host ~]$ echo -n 'aHVudGVyMw==' | base64 --decode
hunter3
```
*Inject Data to Pods*  
You can mount configuration maps and secrets as data volumes or expose the data as environment variables, inside an application container.

CÃ³ 2 cÃ¡ch Ä‘á»ƒ Ä‘Æ°a dá»¯ liá»‡u tá»« ConfigMap hoáº·c Secret vÃ o trong Pod:
- Inject thÃ nh biáº¿n mÃ´i trÆ°á»ng (env).
- Mount thÃ nh file trong container.

1. Inject ConfigMap/Secret thÃ nh Environment Variables

VÃ­ dá»¥ lá»‡nh:
```
oc set env deployment my-deployment --from configmap/my-cm
```
- Ã nghÄ©a: Láº¥y táº¥t cáº£ key/value trong ConfigMap my-cm â†’ inject vÃ o Deployment my-deployment â†’ thÃ nh biáº¿n mÃ´i trÆ°á»ng trong container.
- Káº¿t quáº£ trong pod:
```
env:
  - name: KEY1
    valueFrom:
      configMapKeyRef:
        key: key1
        name: my-cm
```
- DÃ¹ng khi app Ä‘á»c config tá»« ENV.

2. Mount ConfigMap/Secret thÃ nh Volume (file trong container)

VÃ­ dá»¥ lá»‡nh:
```
oc set volume deployment my-deployment --add \
-t secret -m /mnt/secret \
--name myvol --secret-name my-secret
```

Ã nghÄ©a: Mount Secret my-secret vÃ o Deployment my-deployment, gáº¯n táº¡i /mnt/secret.

Káº¿t quáº£ trong pod:
```
volumeMounts:
- mountPath: /mnt/secret
  name: myvol
volumes:
- name: myvol
  secret:
    secretName: my-secret
```

DÃ¹ng khi app cáº§n file (vÃ­ dá»¥: username.txt, password.txt, TLS certâ€¦).

3. LÆ°u Ã½ quan trá»ng
- ConfigMap/Secret chá»‰ dÃ¹ng trong cÃ¹ng namespace (khÃ´ng share giá»¯a project).
- Náº¿u báº¡n cáº­p nháº­t ConfigMap/Secret, pod Ä‘ang cháº¡y khÃ´ng tá»± Ä‘á»™ng nháº­n giÃ¡ trá»‹ má»›i.
  - Báº¡n pháº£i xÃ³a pod hoáº·c rollout láº¡i deployment Ä‘á»ƒ pod má»›i nháº­n giÃ¡ trá»‹ update. Update config thÃ¬ cáº§n tÃ¡i táº¡o pod Ä‘á»ƒ Ã¡p dá»¥ng giÃ¡ trá»‹ má»›i.

1. Service Account (SA) lÃ  gÃ¬?
- LÃ  identity (danh tÃ­nh) cho á»©ng dá»¥ng trong OpenShift.
- CÃ³ thá»ƒ gáº¯n quyá»n RBAC, secrets, SCC (Security Context Constraints)â€¦ vÃ o SA.
- Máº·c Ä‘á»‹nh, má»i pod dÃ¹ng default service account trong namespace.
- Má»—i SA cÃ³ má»™t JWT token Ä‘Æ°á»£c mount vÃ o pod táº¡i:
```
/var/run/secrets/kubernetes.io/serviceaccount
```

â†’ Pod cÃ³ thá»ƒ dÃ¹ng token nÃ y Ä‘á»ƒ gá»i OpenShift API.

2. Táº¡o vÃ  gÃ¡n Service Account
- Táº¡o SA:
```
oc create serviceaccount my-sa
```
- GÃ¡n SA cho deployment:
```
oc set serviceaccount deployment nginx-deployment my-sa
```
Káº¿t quáº£: deployment nginx-deployment sáº½ cháº¡y pod vá»›i SA my-sa.

ğŸ‘‰ DÃ¹ng khi app cáº§n quyá»n Ä‘áº·c biá»‡t (vÃ­ dá»¥: CI/CD pipeline hoáº·c operator gá»i API OpenShift).

3. Security Context & SCC
a. Security Context
- XÃ¡c Ä‘á»‹nh quyá»n, UID, GID, capabilities mÃ  container Ä‘Æ°á»£c phÃ©p.
- VÃ­ dá»¥: cháº¡y non-root, cáº¥m privilege escalation.

b. SCC (Security Context Constraints)
- LÃ  policy báº£o máº­t riÃªng cá»§a OpenShift.
- Tá»± Ä‘á»™ng Ã¡p dá»¥ng security context cho pod.
- Máº·c Ä‘á»‹nh, pod thÆ°á»ng dÃ¹ng SCC restricted-v2.

ğŸ‘‰ Admin cÃ³ thá»ƒ gáº¯n SCC vÃ o SA â†’ Pod nÃ o cháº¡y vá»›i SA Ä‘Ã³ sáº½ Ä‘Æ°á»£c Ã¡p dá»¥ng SCC tÆ°Æ¡ng á»©ng.

4. VÃ­ dá»¥ Deployment vá»›i Security Context
```
securityContext:
  runAsNonRoot: true
  allowPrivilegeEscalation: false
  seccompProfile:
    type: RuntimeDefault
  capabilities:
    drop:
    - ALL
```
- runAsNonRoot: true â†’ cáº¥m cháº¡y user root.
- allowPrivilegeEscalation: false â†’ khÃ´ng cho container tÄƒng quyá»n.
- capabilities.drop: ALL â†’ bá» háº¿t Linux capabilities.
- seccompProfile: RuntimeDefault â†’ dÃ¹ng cáº¥u hÃ¬nh báº£o máº­t máº·c Ä‘á»‹nh cá»§a runtime.

5. LÆ°u Ã½ quan trá»ng
- SA + SCC quyáº¿t Ä‘á»‹nh pod Ä‘Æ°á»£c phÃ©p lÃ m gÃ¬.
- Náº¿u khÃ´ng chá»‰ Ä‘á»‹nh â†’ OpenShift sáº½ Ã¡p dá»¥ng máº·c Ä‘á»‹nh (restricted-v2).
- Äá»ƒ cháº¡y pod Ä‘áº·c biá»‡t (vÃ­ dá»¥ cáº§n quyá»n hostPath, privileged) â†’ cáº§n gÃ¡n SCC phÃ¹ há»£p cho SA.

## 5.5 Deploying Stateful Applications

**Persistent Volumes and Persistent Volume Claims**  
*Persistent Volumes*  
![alt text](pic/24.png)

*Persistent Volume Claims*  
Persistent volume claims (PVCs) represent a request for a persistent volume. These requests can include requirements for the PV, such as the following attributes:
- Amount of storage
- Label selector
- Volume mode
- Access mode
- Storage class

![alt text](pic/25.png)

**Static and Dynamic Provisioning**

1. Static Provisioning
- Admin táº¡o PV thá»§ cÃ´ng trÆ°á»›c â†’ Developer chá»‰ táº¡o PVC Ä‘á»ƒ claim vÃ o PV Ä‘Ã£ cÃ³.
- NhÆ°á»£c Ä‘iá»ƒm:
  - Tá»‘n cÃ´ng quáº£n lÃ½ (Admin pháº£i Ä‘oÃ¡n trÆ°á»›c dung lÆ°á»£ng).
  - Dá»… lÃ£ng phÃ­ náº¿u PV khÃ´ng Ä‘Æ°á»£c dÃ¹ng.
- DÃ¹ng khi storage backend khÃ´ng há»— trá»£ dynamic hoáº·c mÃ´i trÆ°á»ng lab/test.

2. Dynamic Provisioning

- Developer chá»‰ cáº§n táº¡o PVC.
- Cluster sáº½ tá»± Ä‘á»™ng táº¡o PV má»›i phÃ¹ há»£p nhá» StorageClass (SC).
- Äiá»u kiá»‡n:
  - Admin pháº£i cáº¥u hÃ¬nh StorageClass vá»›i má»™t Provisioner plugin (vÃ­ dá»¥: nfs-subdir-external-provisioner, Ceph, EBS, v.v.).
  - Náº¿u default SC chÆ°a Ä‘á»‹nh nghÄ©a, PVC pháº£i chá»‰ rÃµ storageClassName.

3. StorageClass (SC)
- LÃ  â€œprofileâ€ cá»§a storage: Ä‘á»‹nh nghÄ©a loáº¡i storage, tá»‘c Ä‘á»™, reclaim policy,â€¦
- VÃ­ dá»¥ SC:
```
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: nfs-storage
  annotations:
    storageclass.kubernetes.io/is-default-class: "true"
provisioner: k8s-sigs.io/nfs-subdir-external-provisioner
reclaimPolicy: Delete
volumeBindingMode: Immediate
```
- provisioner: plugin lo viá»‡c cáº¥p phÃ¡t storage.
- reclaimPolicy: Delete (xoÃ¡ PV khi xoÃ¡ PVC) hoáº·c Retain (giá»¯ láº¡i PV).
- is-default-class: "true" â†’ PVC nÃ o khÃ´ng ghi storageClassName thÃ¬ máº·c Ä‘á»‹nh dÃ¹ng SC nÃ y.

4. PVC vá»›i Dynamic Provisioning

VÃ­ dá»¥:
```
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: dynamic-volume-claim
spec:
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 1Gi
  storageClassName: nfs-storage
```
- Khi táº¡o PVC nÃ y â†’ SC nfs-storage sáº½ táº¡o ra má»™t PV má»›i tá»± Ä‘á»™ng â†’ PVC Ä‘Æ°á»£c bind.
- Admin khÃ´ng cáº§n táº¡o PV thá»§ cÃ´ng.

So sÃ¡nh
| Äáº·c Ä‘iá»ƒm    | Static Provisioning   | Dynamic Provisioning            |
| ----------- | --------------------- | ------------------------------- |
| Ai táº¡o PV   | Admin                 | Tá»± Ä‘á»™ng (qua StorageClass)      |
| Ai táº¡o PVC  | Developer             | Developer                       |
| Linh hoáº¡t   | KÃ©m                   | Cao                             |
| ThÆ°á»ng dÃ¹ng | Test, storage cá»‘ Ä‘á»‹nh | Production, CI/CD, Cloud-native |

**Mounting Claims Within Pods**

Mount PVC vÃ o Pod

![alt text](pic/26.png)

Mount PVC vÃ o Deployment

![alt text](pic/27.png)

Káº¿t quáº£: Deployment YAML Ä‘Æ°á»£c cáº­p nháº­t thÃªm:
```bash
volumeMounts:
- mountPath: /tmp/data
  name: nfs-volume-storage

volumes:
- name: nfs-volume-storage
  persistentVolumeClaim:
    claimName: my-data-claim
```
3. LÆ°u Ã½ khi dÃ¹ng PVC trong Deployment
- Táº¥t cáº£ pod replica trong Deployment Ä‘á»u mount cÃ¹ng PVC â†’ cÃ³ thá»ƒ gÃ¢y conflict náº¿u á»©ng dá»¥ng khÃ´ng há»— trá»£ shared storage.
- Vá»›i database (MySQL, PostgreSQL, MongoDB, â€¦):
  - Má»—i instance thÆ°á»ng cáº§n storage riÃªng.
  - Replication/sharding do chÃ­nh database quáº£n lÃ½, khÃ´ng dÃ¹ng chung PVC.
- Vá»›i stateless app (web server, API, â€¦): thÆ°á»ng khÃ´ng cáº§n PVC â†’ dá»… scale hÆ¡n.

4. Ephemeral Storage (táº¡m thá»i)

- DÃ¹ng volume Ä‘á»ƒ inject file cÃ³ sáºµn mÃ  khÃ´ng cáº§n rebuild image.
- VÃ­ dá»¥:
  - Script init database.
  - Config server.
  - TLS cert/token/key.
- ConfigMap & Secret cÅ©ng cÃ³ thá»ƒ mount dÆ°á»›i dáº¡ng volume â†’ ráº¥t phá»• biáº¿n.
- Dá»¯ liá»‡u ephemeral sáº½ máº¥t khi pod bá»‹ xoÃ¡ (khÃ´ng persistent).

**Stateful Sets**

As their name suggests, stateful sets are intended for stateful applications. Unlike deployments, pods within stateful sets are guaranteed to have a predictable identifier (ID) for each pod. For example, three replicas for the redis stateful set might use the redis-0, redis-1, and redis-2 pod names. This is useful for routing requests to each pod or discovering new pods that must join to an existing cluster. In addition, each replica pod created by a stateful set can have a dedicated PVC.

![alt text](pic/28.png)
1. BÃ¬nh thÆ°á»ng (Service cÃ³ ClusterIP)
- Khi báº¡n táº¡o Service bÃ¬nh thÆ°á»ng (ClusterIP â‰  None), Kubernetes/OpenShift sáº½ load balance traffic Ä‘áº¿n báº¥t ká»³ Pod nÃ o match label selector.
- CÃ¡c Pod trong Deployment thÆ°á»ng giá»‘ng há»‡t nhau, nÃªn viá»‡c â€œgá»­i vÃ o Pod nÃ o cÅ©ng Ä‘Æ°á»£câ€ â†’ khÃ´ng váº¥n Ä‘á».

2. Headless Service (ClusterIP = None)
```
apiVersion: v1
kind: Service
metadata:
  name: my-stateful-app
spec:
  clusterIP: None
  selector:
    app: my-stateful-app
```
- clusterIP: None â†’ Service khÃ´ng cÃ³ IP riÃªng, khÃ´ng lÃ m load balancing.
- Thay vÃ o Ä‘Ã³, má»—i Pod trong StatefulSet cÃ³ DNS riÃªng.
- VÃ­ dá»¥ StatefulSet táº¡o 3 replica:
```
my-stateful-app-0.my-stateful-app

my-stateful-app-1.my-stateful-app

my-stateful-app-2.my-stateful-app
```
Má»—i Pod cÃ³ Ä‘á»‹a chá»‰ riÃªng, cÃ³ thá»ƒ gá»i trá»±c tiáº¿p.

## Monitoring Application Health

*Specifying Application Resource Requirements*

![alt text](image.png)

**Types of Probes**  
1. Startup Probe
- Kiá»ƒm tra: á»©ng dá»¥ng trong container Ä‘Ã£ khá»Ÿi Ä‘á»™ng thÃ nh cÃ´ng chÆ°a.
- Chá»‰ cháº¡y lÃºc startup, 1 láº§n cho Ä‘áº¿n khi thÃ nh cÃ´ng â†’ sau Ä‘Ã³ má»›i kÃ­ch hoáº¡t cÃ¡c probe khÃ¡c.
- Náº¿u fail â†’ container bá»‹ kill vÃ  restart (theo restartPolicy).
- DÃ¹ng khi: á»©ng dá»¥ng khá»Ÿi Ä‘á»™ng lÃ¢u (VD: Java app, DB serverâ€¦).

ğŸ‘‰ Cáº¥u hÃ¬nh: spec.containers.startupProbe

2. Readiness Probe

- Kiá»ƒm tra: container cÃ³ sáºµn sÃ ng nháº­n traffic chÆ°a.
- Náº¿u fail â†’ Pod bá»‹ loáº¡i khá»i Service endpoint, khÃ´ng nháº­n request cho Ä‘áº¿n khi pass.
- DÃ¹ng khi: á»©ng dá»¥ng cáº§n lÃ m viá»‡c chuáº©n bá»‹ (má»Ÿ káº¿t ná»‘i DB, load config/cache).
- KhÃ´ng kill container, chá»‰ â€œÄ‘á»©ng ngoÃ i load balancerâ€.

ğŸ‘‰ Cáº¥u hÃ¬nh: spec.containers.readinessProbe

3. Liveness Probe
- Kiá»ƒm tra: á»©ng dá»¥ng trong container cÃ²n cháº¡y khá»e máº¡nh khÃ´ng.
- Náº¿u fail â†’ OpenShift/K8s sáº½ restart container.
- DÃ¹ng khi: á»©ng dá»¥ng cÃ³ thá»ƒ bá»‹ treo (deadlock, memory leak).

ğŸ‘‰ Cáº¥u hÃ¬nh: spec.containers.livenessProbe

![alt text](pic/30.png)

**Methods of Checking Application Health**  
Startup, readiness, and liveness probes can verify that an application is in a healthy state in the following three ways:
- HTTP checks
- Container execution checks
- TCP socket checks

HTTP Checks
![alt text](pic/31.png)

Container Execution Checks

![alt text](pic/32.png)

TCP Socket Checks

![alt text](pic/33.png)

**Managing Probes**  
Developers can create and manage probes with either the oc CLI client or the OpenShift web console.

![alt text](pic/34.png)

![alt text](pic/35.png)

**Probes Via the CLI**  
The oc set probe command creates probes on existing workloads.
```
[user@host ~]$ oc set probe deployment/myapp \
--readiness \
--get-url=http://:8080/readyz \
--period-seconds=20
```
```
[user@host ~]$ oc set probe deployment/myapp \
--liveness \
--open-tcp=3306 \
--period-seconds=20 \
--timeout-seconds=1
```
```
[user@host ~]$ oc set probe deployment/myapp \
--liveness \
--get-url=http://:8080/livez \
--initial-delay-seconds=30 \
--success-threshold=1 \
--failure-threshold=3
```
Use the oc set probe --help command to view the available options.

**Automatically Scaling Applications**   
Horizontal Scaling

![alt text](pic/36.png)

Vertical Scaling


---
# Chapter 6.  Deploying Multi-container Applications

ğŸ¯ OpenShift Template dÃ¹ng Ä‘á»ƒ lÃ m gÃ¬?
- ÄÃ³ng gÃ³i nhiá»u resource láº¡i thÃ nh 1 gÃ³i â†’ dá»… triá»ƒn khai.
VÃ­ dá»¥: Deployment + Service + Route + PVC â†’ gom chung vÃ o 1 file Template.
- DÃ¹ng tham sá»‘ (parameters) â†’ linh hoáº¡t khi deploy.
VÃ­ dá»¥: thay Ä‘á»•i tÃªn app, image, sá»‘ replicas, hoáº·c kÃ­ch thÆ°á»›c storage mÃ  khÃ´ng cáº§n sá»­a tay nhiá»u chá»— trong YAML.
- Tá»± Ä‘á»™ng hÃ³a vÃ  tÃ¡i sá»­ dá»¥ng â†’ QA, Dev, Ops chá»‰ cáº§n 1 lá»‡nh oc new-app -f template.yaml -p PARAM=VALUE lÃ  deploy ra nguyÃªn cá»¥m app.
- DÃ¹ng cho mÃ´i trÆ°á»ng multi-tier (web + app + DB) â†’ deploy nhanh cáº£ há»‡ thá»‘ng thay vÃ¬ tá»«ng bÆ°á»›c.
- ISV (nhÃ  cung cáº¥p pháº§n má»m) hay dÃ¹ng template Ä‘á»ƒ ship sáº£n pháº©m â†’ khÃ¡ch hÃ ng chá»‰ cáº§n cháº¡y template lÃ  cÃ³ Ä‘á»§ cÃ¡c thÃ nh pháº§n.

![alt text](pic/37.png)

**Creating an Application from a Template**  
You can deploy an application directly from a template definition file. The oc new-app and oc process commands can use a template as an input and process this file to create resources.

The oc new-app command can process a template file to create resources in OpenShift, as follows:
```
[user@host ~]$ oc new-app --file mytemplate.yaml -p PARAM1=value1 \
-p PARAM2=value2
```
You can also use a template stored in the cluster. This example processes a template called mysql-persistent:
```
[user@host ~]$ oc new-app --template mysql-persistent \
-p MYSQL_USER=student -p MYSQL_PASSWORD=mypass
```
The oc process command processes a template and produces a resource list. You can save the resource list to a local file as follows:
```
[user@host ~]$ oc process -f mytemplate.yaml -p PARAM1=value1 \
-p PARAM2=value2 > myresourcelist.json
```
Note
You can modify the output format, which is JSON by default, by using the --output (-o) option.

## 6.3 Install Applications by Using Helm Charts

![alt text](pic/38.png)

**Deploy Charts with Web Console**

**Manage Charts with Helm CLI**
```
[user@host ~]$ helm repo add openshift-helm-charts \
https://charts.openshift.io/
"openshift-helm-charts" has been added to your repositories
```
When you add a private registry, you can define the credentials when adding the repository, for example by using the `--username` and `--password` flags.

You can search the contents of the repository:
```
[user@host ~]$ helm search repo openshift-helm-charts
NAME                                              	CHART VERSION	APP VERSION 	DESCRIPTION
openshift-helm-charts/a10tkc                      	0.2.0        	1.16.0        A Helm chart for A10 Thunder Kubernetes Connector
openshift-helm-charts/akeyless-api-gateway        	1.41.2       	4.4.1         A Helm chart for Kubernetes that deploys akeyle...
openshift-helm-charts/alaz                        	0.5.0        	v0.5.2        Alaz is an open-source Ddosify eBPF agent that ...
...output omitted...
```

Use the `helm pull` command to download a helm chart.
```
[user@host ~]$ helm pull openshift-helm-charts/redhat-quarkus \
--untar --destination redhat-quarkus
...no output expected...
```

To upload your charts, use the helm package command to create a chart tar file.
```
[user@host ~]$ helm package my-chart-directory
Successfully packaged chart and saved it to: /home/user/example-chart-0.1.0.tgz
```
Then, use the helm push command to upload the packaged helm chart to your repository.
```
[user@host ~]$ helm push example-chart-0.1.0.tgz example.repository.org
...output omitted...
```
Finally, use the helm install command to install a remote or local chart.
```
[user@host ~]$ helm install my-quarkus-application \
openshift-helm-charts/redhat-quarkus \
--set replicaCount=3,image.tag=latest
...output omitted...
Your Quarkus app is building! To view the build logs, run:

oc logs bc/my-quarkus-application --follow
...output omitted...
```

So sanh
| Lá»‡nh | Má»¥c Ä‘Ã­ch |
| --- | --- |
| `helm pull`    | Chá»‰ **táº£i chart** vá» local, chÆ°a cÃ i gÃ¬ lÃªn cluster.   |
| `helm install` | **Triá»ƒn khai chart** thÃ nh á»©ng dá»¥ng tháº­t trÃªn cluster. |

You can view Helm releases, or applications that you installed by using Helm charts:
```
[user@host ~]$ helm ls
NAME                  	NAMESPACE  REVISION	UPDATED   STATUS  	CHART
my-quarkus-application	test	     1       	2022-...	deployed	quarkus-0.0.3
vertx-app             	test	     1       	2023-...	deployed	ver
```
When the chart developer releases an updated chart, you can update your release:
```
[user@host ~]$ helm upgrade my-quarkus-application \
openshift-helm-charts/redhat-quarkus
Release "my-quarkus-application" has been upgraded. Happy Helming!
NAME: my-quarkus-application
LAST DEPLOYED: Thu Aug 17 08:37:07 2023
NAMESPACE: multicontainer-helm
STATUS: deployed
REVISION: 2
TEST SUITE: None
```
Use helm history to list the release application history:
```
[user@host ~]$ helm history my-quarkus-application
...output omitted...
REVISION	UPDATED                 	STATUS    	CHART        	... 	DESCRIPTION
1       	Thu Jun 06 08:36:13 2022	superseded	quarkus-0.0.2	 	    Install complete
2       	Thu Aug 17 08:37:07 2023	deployed  	quarkus-0.0.3	 	    Upgrade complete
```
If the new chart version does not work as you expect, you can roll back to the previous version. The following example rolls back the my-quarkus-application release to revision 1.
```
[user@host ~]$ helm rollback my-quarkus-application 1
Rollback was a success! Happy Helming!
```
Finally, you can uninstall the release:
```
[user@host ~]$ helm uninstall my-quarkus-application
release "my-quarkus-application" uninstalled
```

**Creating Helm Charts**
To generate a Helm chart directory structure, use the helm create command:
```
[user@host ~]$ helm create my-helm-chart
Creating my-helm-chart
```
The previous command creates the following directory structure:
```
[user@host ~]$ tree my-helm-chart
my-helm-chart/
â”œâ”€â”€ Chart.yaml
â”œâ”€â”€ charts
â”œâ”€â”€ templates
â”‚   â”œâ”€â”€ NOTES.txt
â”‚   â”œâ”€â”€ _helpers.tpl
â”‚   â”œâ”€â”€ deployment.yaml
â”‚   â”œâ”€â”€ hpa.yaml
â”‚   â”œâ”€â”€ ingress.yaml
â”‚   â”œâ”€â”€ service.yaml
â”‚   â”œâ”€â”€ serviceaccount.yaml
â”‚   â””â”€â”€ tests
â”‚       â””â”€â”€ test-connection.yaml
â””â”€â”€ values.yaml
```
Charts contain the following important components:

`Chart.yaml`   
This is the main chart file that contains the chart metadata. For example, it defines the chart name, its description, and version.

`values.yaml`  
The values.yaml file contains variables that you can use to template your YAML files, for example:
```
replicaCount: 1

image:
  repository: nginx
  pullPolicy: IfNotPresent
```
`templates`  
The templates directory holds the YAML files that you want to template and deploy. By default, Helm deploys all YAML files that are present in this directory.

`templates/NOTES.txt`  
This file configures the text that Helm prints after you install the chart. Typically, this file contains information about the deployed application, such as the application URL, or information about how developers can interact with the application.

**Verify Templates**  
When you create templates, it is useful to verify that the templates are syntactically correct, which means that Helm can render the template. Use the helm template command to render all templates in the chart, for example:
```
[user@host ~]$ helm template my-helm-chart
---
# Source: my-helm-chart/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
...output omitted...
```
To render a specific template, use the --show-only parameter. The short -s parameter is an alternative to the --show-only parameter.
```
[user@host ~]$ helm template -s templates/serviceaccount.yaml my-helm-chart
---
# Source: my-helm-chart/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
...output omitted...
```
Similarly to installing a chart, you can template a remote chart as well, for example:
```
[user@host ~]$ helm template openshift-helm-charts/redhat-quarkus
---
# Source: quarkus/templates/service.yaml
apiVersion: v1
kind: Service
...output omitted...
```
This is useful to inspect the chart YAML files before deploying the chart without downloading the chart locally.

**Template YAML Files With Helm**

ğŸ”¹ CÃ¡ch hoáº¡t Ä‘á»™ng
- Báº¡n cÃ³ `values.yaml`: chá»©a cÃ¡c biáº¿n cáº¥u hÃ¬nh.
- Báº¡n cÃ³ file trong templates/: chá»©a YAML + cÃº phÃ¡p template ({{ }}) Ä‘á»ƒ láº¥y giÃ¡ trá»‹ tá»« values.yaml.
- Khi cháº¡y helm install, Helm sáº½ thay tháº¿ biáº¿n báº±ng giÃ¡ trá»‹ tháº­t tá»« values.yaml rá»“i apply.

VÃ­ dá»¥ 1: DÃ¹ng biáº¿n cÆ¡ báº£n

ğŸ“„ values.yaml
```
replicaCount: 3
image:
  repository: quay.io/example/deployment
  tag: "1.0"
```

ğŸ“„ templates/deployment.yaml
```
apiVersion: apps/v1
kind: Deployment
metadata:
  name: example-deployment
spec:
  replicas: {{ .Values.replicaCount }}
  template:
    spec:
      containers:
      - name: example-deployment
        image: {{ .Values.image.repository }}:{{ .Values.image.tag }}
```

ğŸ‘‰ Khi render, Helm táº¡o ra Deployment vá»›i 3 replicas, image Ä‘Ãºng nhÆ° trong values.yaml.

VÃ­ dá»¥ 2: DÃ¹ng with
```
{{ with .Values.image }}
containers:
- name: example-deployment
  image: {{ .repository }}:{{ .tag }}
{{ end }}
```

á» Ä‘Ã¢y, with .Values.image nghÄ©a lÃ  báº¡n zoom vÃ o scope image. BÃªn trong, .repository = image.repository.

VÃ­ dá»¥ 3: DÃ¹ng hÃ m built-in
```
imagePullPolicy: {{ .Values.image.pullPolicy | default "Always" | quote }}
```
- default "Always": náº¿u pullPolicy khÃ´ng cÃ³, thÃ¬ dÃ¹ng giÃ¡ trá»‹ máº·c Ä‘á»‹nh "Always".
- quote: bá»c chuá»—i trong dáº¥u ".

VÃ­ dá»¥ 4: DÃ¹ng Ä‘iá»u kiá»‡n if
```
{{ if eq .Values.createSharedSecret "true" }}
env:
  - name: DATABASE_USER
    valueFrom:
      secretKeyRef:
        name: postgresql
        key: database-user
{{ end }}
```

ğŸ‘‰ NghÄ©a lÃ : chá»‰ thÃªm env vÃ o container náº¿u trong values.yaml cÃ³:
```
createSharedSecret: "true"
```

Náº¿u false hoáº·c khÃ´ng cÃ³ â†’ Ä‘oáº¡n YAML Ä‘Ã³ sáº½ bá»‹ bá» qua.

âœ… TÃ³m láº¡i:
- Helm template cho phÃ©p biáº¿n hÃ³a YAML Ä‘á»™ng: thÃªm/bá»›t, thay giÃ¡ trá»‹, Ä‘iá»u kiá»‡nâ€¦
- Nhá» Ä‘Ã³ 1 chart cÃ³ thá»ƒ deploy cho nhiá»u mÃ´i trÆ°á»ng (dev, staging, prod) chá»‰ báº±ng cÃ¡ch Ä‘á»•i values.yaml, khÃ´ng cáº§n viáº¿t láº¡i YAML tá»« Ä‘áº§u.

## 6.5 The Kustomize CLI

The following directory structure shows an example of a Kustomize directory layout:
```
myapp
â”œâ”€â”€ base
â””â”€â”€ overlays
  â”œâ”€â”€ production
  â””â”€â”€ staging
```

Resources Files
The resources section of the kustomization.yaml file is a list of files that create all the resources for a specific environment, for example:

resources:
- deployment.yaml
- secrets.yaml
- service.yaml
The layout for a base definition with the preceding resources uses the following directory structure:
```
myapp
â””â”€â”€ base
  â”œâ”€â”€ deployment.yaml
  â”œâ”€â”€ kustomization.yaml
  â”œâ”€â”€ secrets.yaml
  â””â”€â”€ service.yaml
```
Separating Kubernetes object definitions into smaller files simplifies maintenance of the base set.

Overlay Configuration
The kustomization.yaml file in an overlay directory must point to one or multiple base configuration sets that form the starting point for the overlay, for example:
```
resources:
- ../../base
```
This kustomization.yaml file is placed in an overlay, for example:
```
myapp/
â”œâ”€â”€ base
â”‚   â”œâ”€â”€ deployment.yaml
â”‚   â”œâ”€â”€ kustomization.yaml
â”‚   â”œâ”€â”€ secrets.yaml
â”‚   â””â”€â”€ service.yaml
â””â”€â”€ overlays
    â””â”€â”€ staging
        â””â”€â”€ kustomization.yaml
```
This overlay starts with the base configuration and then applies any patch defined in it.  
**Multi-container Deployments Comparison**  
![alt text](pic/39.png)

---
# Chapter 7.  Continuous Deployment by Using Red Hat OpenShift Pipelines

![alt text](pic/40.png)


Pipelines Workflow

![alt text](pic/41.png)

To start a pipeline or a task, you can use the tkn CLI or the Pipelines section of the Web console, available both in the developer and administrator perspectives.

![alt text](pic/43.png)

**Differences with Jenkins**
![alt text](pic/42.png)


## Creating CI/CD Workflows by Using Red Hat OpenShift Pipelines
**Defining Custom Tasks**

![alt text](pic/44.png)

Default Tasks

```
[user@host ~]$ oc get task/buildah \
-n openshift-pipelines \
-o yaml
apiVersion: tekton.dev/v1
kind: Task
metadata:
  name: buildah
```

Alternatively, you can use the Tekton CLI (tkn). The following example command shows metadata about the same buildah task.
```
[user@host ~]$ tkn -n openshift-pipelines t describe buildah
Name:          buildah
Namespace:     openshift-pipelines
Description:   Buildah task builds source into a container image and
then pushes it to a container registry.
...output omitted...

Params
```

![alt text](pic/45.png)

![alt text](pic/46.png)













